{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 导入库|"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aa8e8bcef1154f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 导入pytoch并检查是否为GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bea61676269513f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-02T03:35:53.238269400Z",
     "start_time": "2024-01-02T03:35:42.144201300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T03:42:40.174901800Z",
     "start_time": "2024-01-02T03:42:40.155904100Z"
    }
   },
   "id": "f66552bf3ff2eb43"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "combined_array = np.load('out_data/combined_array.npy')\n",
    "\n",
    "total_energy = np.load('out_data/total_energy.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T03:42:41.066715500Z",
     "start_time": "2024-01-02T03:42:41.041723100Z"
    }
   },
   "id": "dd29785e034207e4"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.13779316e+04, 2.72026625e+05, 1.63766479e+02, ...,\n        5.05832761e+00, 3.92486575e+00, 2.83953478e+00],\n       [1.66335781e+04, 3.85932219e+05, 1.96714127e+02, ...,\n        1.42945050e+00, 4.23036947e+00, 2.89753474e+00],\n       [1.66431816e+04, 3.86002562e+05, 1.96775146e+02, ...,\n        4.74763596e+00, 3.93023353e+00, 2.84459229e+00],\n       ...,\n       [1.12181348e+04, 2.60452359e+05, 1.60908371e+02, ...,\n        1.36060946e+01, 3.45040872e+00, 2.76416725e+00],\n       [1.12181338e+04, 2.60452359e+05, 1.60908356e+02, ...,\n        1.36070126e+01, 3.45040643e+00, 2.76417683e+00],\n       [1.24861270e+04, 2.94673500e+05, 1.69455948e+02, ...,\n        2.06497536e+01, 3.18603619e+00, 2.72785907e+00]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T04:26:09.294654600Z",
     "start_time": "2024-01-02T04:26:09.258837900Z"
    }
   },
   "id": "53d46d3b55d2b9dc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "with open('out_data/total_elec.pkl', 'rb') as file:\n",
    "    total_elec = pickle.load(file)\n",
    "with open('out_data/total_spms.pkl', 'rb') as file:\n",
    "    total_spms = pickle.load(file)\n",
    "with open('out_data/atom_properties.pkl', 'rb') as file:\n",
    "    atom_properties = pickle.load(file)\n",
    "with open('out_data/total_out_basics.pkl', 'rb') as file:\n",
    "    total_out_basics = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T03:47:03.348498500Z",
     "start_time": "2024-01-02T03:47:03.216849400Z"
    }
   },
   "id": "5a7659c0ab39b042"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2d 3d数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a337875d7dc45631"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 8, 8, 8, 8) (291, 8, 20, 40) (291, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "elec = []\n",
    "spms = []\n",
    "atom = []\n",
    "for i in range(len(total_out_basics)):\n",
    "    indices = [index - 1 for index in total_out_basics[i][0]]\n",
    "    \n",
    "    elec_values = [total_elec[i][index] for index in indices]\n",
    "    elec.append(elec_values)\n",
    "\n",
    "    spms_values = [total_spms[i][index] for index in indices]\n",
    "    spms.append(spms_values)\n",
    "    \n",
    "    atom_values = [atom_properties[i][index] for index in indices]\n",
    "    atom.append(atom_values)\n",
    "elec = np.array(elec)\n",
    "spms = np.array(spms)\n",
    "atom = np.array(atom)\n",
    "\n",
    "print(elec.shape,spms.shape, atom.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T04:27:20.030887500Z",
     "start_time": "2024-01-02T04:27:19.978245700Z"
    }
   },
   "id": "1060b2d3ccbaeaeb"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader ,random_split\n",
    "\n",
    "class ComplexDataset(Dataset):\n",
    "    def __init__(self, elec, spms, atom,combined_array, energy):\n",
    "        self.elec = torch.tensor(elec, dtype=torch.float32)  # 将elec转换为张量\n",
    "        self.spms = torch.tensor(spms, dtype=torch.float32)  # 将spms转换为张量\n",
    "        self.atom = torch.tensor(atom, dtype=torch.float32)  # atom转换为张量\n",
    "        self.molecular = torch.tensor(combined_array, dtype=torch.float32)  # atom转换为张量\n",
    "        self.energy = torch.tensor(energy, dtype=torch.float32)  # 将energy转换为张量\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energy)  # 返回样本总数\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引返回单个样本\n",
    "        return self.elec[idx], self.spms[idx], self.atom[idx], self.molecular[idx], self.energy[idx]\n",
    "\n",
    "# 创建ComplexDataset实例\n",
    "# dataset = ComplexDataset(elec, spms, atom,combined_array, total_energy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:35:26.280582700Z",
     "start_time": "2024-01-02T06:35:26.240510800Z"
    }
   },
   "id": "bb807bf68be6418f"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "with open('test_dataset/out_data/total_elec.pkl', 'rb') as file:\n",
    "    total_elec2 = pickle.load(file)\n",
    "with open('test_dataset/out_data/total_spms.pkl', 'rb') as file:\n",
    "    total_spms2 = pickle.load(file)\n",
    "with open('test_dataset/out_data/atom_properties.pkl', 'rb') as file:\n",
    "    atom_properties2 = pickle.load(file)\n",
    "with open('test_dataset/out_data/total_out_basics.pkl', 'rb') as file:\n",
    "    total_out_basics2 = pickle.load(file)\n",
    "combined_array2 = np.load('test_dataset/out_data/combined_array.npy')\n",
    "\n",
    "total_energy2 = np.load('test_dataset/out_data/total_energy.npy')\n",
    "\n",
    "\n",
    "elec2 = []\n",
    "spms2 = []\n",
    "atom2 = []\n",
    "for i in range(len(total_out_basics2)):\n",
    "    indices = [index - 1 for index in total_out_basics2[i][0]]\n",
    "    \n",
    "    elec_values = [total_elec2[i][index] for index in indices]\n",
    "    elec2.append(elec_values)\n",
    "\n",
    "    spms_values = [total_spms2[i][index] for index in indices]\n",
    "    spms2.append(spms_values)\n",
    "    \n",
    "    atom_values = [atom_properties2[i][index] for index in indices]\n",
    "    atom2.append(atom_values)\n",
    "elec2 = np.array(elec2)\n",
    "spms2 = np.array(spms2)\n",
    "atom2 = np.array(atom2)\n",
    "\n",
    "print(elec2.shape,spms2.shape, atom2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:26:32.671941200Z",
     "start_time": "2024-01-02T06:26:32.558100800Z"
    }
   },
   "id": "8136129967511e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d0e6404408cc70"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "dataset = ComplexDataset(elec, spms, atom,combined_array, total_energy)\n",
    "\n",
    "test_dataset = ComplexDataset(elec2, spms2, atom2,combined_array2, total_energy2)\n",
    "# 在创建DataLoader时使用\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:38:52.494997400Z",
     "start_time": "2024-01-02T06:38:52.434804500Z"
    }
   },
   "id": "cbb6f4decb3a6073"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimplifiedConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedConvNet, self).__init__()\n",
    "        # 2D数据的卷积层\n",
    "        self.conv2d_1 = nn.Conv2d(8, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2d_2 = nn.Conv2d(32, 64, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(64)\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3D数据的卷积层\n",
    "        self.conv3d_1 = nn.Conv3d(8, 32, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn3d_1 = nn.BatchNorm3d(32)\n",
    "        self.maxpool3d_1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3d_2 = nn.Conv3d(32, 64, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn3d_2 = nn.BatchNorm3d(64)\n",
    "        self.maxpool3d_2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc_1d1 = nn.Linear(8 * 8, 128)\n",
    "        self.fc_1d2 = nn.Linear(128, 256)\n",
    "        self.bn1d_1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(12)\n",
    "        \n",
    "        \n",
    "        self.dropout2d = nn.Dropout(p=0.4)\n",
    "        self.dropout3d = nn.Dropout(p=0.5)\n",
    "        self.dropout1d = nn.Dropout(p=0.1)\n",
    "        self.dropout0d = nn.Dropout(p=0.05)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.weight_2d = nn.Parameter(torch.ones(1))\n",
    "        self.weight_3d = nn.Parameter(torch.ones(1))\n",
    "        self.weight_1d = nn.Parameter(torch.ones(1))\n",
    "        self.weight_0d = nn.Parameter(torch.ones(1))\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(512 + 3200 + 256 +12, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.fc6 = nn.Linear(64, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x3d, x2d, x1d ,x0d):\n",
    "        \n",
    "        # 处理3D数据\n",
    "        x3d = F.relu(self.bn3d_1(self.conv3d_1(x3d)))\n",
    "        x3d = self.maxpool3d_1(x3d)\n",
    "        x3d = F.relu(self.bn3d_2(self.conv3d_2(x3d)))\n",
    "        x3d = self.maxpool3d_2(x3d)\n",
    "        x3d = x3d.flatten(1)\n",
    "        \n",
    "        # 处理2D数据\n",
    "        x2d = F.relu(self.bn2d_1(self.conv2d_1(x2d)))\n",
    "        x2d = self.maxpool2d_1(x2d)\n",
    "        x2d = F.relu(self.bn2d_2(self.conv2d_2(x2d)))\n",
    "        x2d = self.maxpool2d_2(x2d)\n",
    "        x2d = x2d.flatten(1)\n",
    "\n",
    "        # 处理1D数据\n",
    "        x1d = x1d.view(x1d.size(0), -1)\n",
    "        x1d = F.relu(self.fc_1d1(x1d))\n",
    "        x1d = F.relu(self.fc_1d2(x1d))\n",
    "        x1d = self.bn1d_1(x1d)\n",
    "\n",
    "        x0d = self.bn0(x0d)\n",
    "\n",
    "        x3d = self.dropout3d(x3d)\n",
    "        x2d = self.dropout2d(x2d)\n",
    "        x1d = self.dropout1d(x1d)\n",
    "        x0d = self.dropout0d(x0d)\n",
    "\n",
    "\n",
    "        weighted_3d = self.weight_2d * x3d\n",
    "        weighted_2d = self.weight_3d * x2d\n",
    "        weighted_1d = self.weight_1d * x1d\n",
    "        weighted_0d = self.weight_0d * x0d\n",
    "        \n",
    "        combined = torch.cat((weighted_3d, weighted_2d, weighted_1d, weighted_0d), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:34:44.097160500Z",
     "start_time": "2024-01-02T06:34:44.049158600Z"
    }
   },
   "id": "d000617c9cd0710b"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 683.5001, Test Loss: 71.8016\n",
      "Epoch [2/1000], Training Loss: 76.1317, Test Loss: 47.7777\n",
      "Epoch [3/1000], Training Loss: 55.7123, Test Loss: 18.5472\n",
      "Epoch [4/1000], Training Loss: 46.6323, Test Loss: 27.7298\n",
      "Epoch [5/1000], Training Loss: 60.3479, Test Loss: 101.8864\n",
      "Epoch [6/1000], Training Loss: 55.3021, Test Loss: 70.6981\n",
      "Epoch [7/1000], Training Loss: 47.5306, Test Loss: 433.8249\n",
      "Epoch [8/1000], Training Loss: 67.2500, Test Loss: 19.1521\n",
      "Epoch [9/1000], Training Loss: 112.6141, Test Loss: 116.6367\n",
      "Epoch [10/1000], Training Loss: 55.7968, Test Loss: 126.9799\n",
      "Epoch [11/1000], Training Loss: 48.8314, Test Loss: 9.8331\n",
      "Epoch [12/1000], Training Loss: 44.5098, Test Loss: 8.7931\n",
      "Epoch [13/1000], Training Loss: 33.7198, Test Loss: 28.3560\n",
      "Epoch [14/1000], Training Loss: 57.5289, Test Loss: 27.0064\n",
      "Epoch [15/1000], Training Loss: 62.2241, Test Loss: 86.7103\n",
      "Epoch [16/1000], Training Loss: 82.7556, Test Loss: 17.5777\n",
      "Epoch [17/1000], Training Loss: 43.7670, Test Loss: 24.1723\n",
      "Epoch [18/1000], Training Loss: 48.8942, Test Loss: 9.8239\n",
      "Epoch [19/1000], Training Loss: 38.4433, Test Loss: 113.5206\n",
      "Epoch [20/1000], Training Loss: 49.6547, Test Loss: 250.0280\n",
      "Epoch [21/1000], Training Loss: 54.5617, Test Loss: 103.1723\n",
      "Epoch [22/1000], Training Loss: 42.1875, Test Loss: 49.5157\n",
      "Epoch [23/1000], Training Loss: 25.0850, Test Loss: 47.9182\n",
      "Epoch [24/1000], Training Loss: 22.8957, Test Loss: 88.8957\n",
      "Epoch [25/1000], Training Loss: 25.8250, Test Loss: 75.4819\n",
      "Epoch [26/1000], Training Loss: 24.2522, Test Loss: 110.0955\n",
      "Epoch [27/1000], Training Loss: 30.2228, Test Loss: 117.9472\n",
      "Epoch [28/1000], Training Loss: 24.1883, Test Loss: 133.4351\n",
      "Epoch [29/1000], Training Loss: 21.9047, Test Loss: 102.7422\n",
      "Epoch [30/1000], Training Loss: 17.3968, Test Loss: 70.2914\n",
      "Epoch [31/1000], Training Loss: 24.8491, Test Loss: 52.6016\n",
      "Epoch [32/1000], Training Loss: 34.3578, Test Loss: 72.5822\n",
      "Epoch [33/1000], Training Loss: 24.6617, Test Loss: 25.9591\n",
      "Epoch [34/1000], Training Loss: 13.1867, Test Loss: 53.9356\n",
      "Epoch [35/1000], Training Loss: 25.5099, Test Loss: 80.3267\n",
      "Epoch [36/1000], Training Loss: 15.6736, Test Loss: 127.0992\n",
      "Epoch [37/1000], Training Loss: 22.4330, Test Loss: 71.1965\n",
      "Epoch [38/1000], Training Loss: 36.7938, Test Loss: 131.3955\n",
      "Epoch [39/1000], Training Loss: 22.7256, Test Loss: 106.4006\n",
      "Epoch [40/1000], Training Loss: 17.6419, Test Loss: 64.5938\n",
      "Epoch [41/1000], Training Loss: 23.2829, Test Loss: 793.9920\n",
      "Epoch [42/1000], Training Loss: 26.9983, Test Loss: 79.8839\n",
      "Epoch [43/1000], Training Loss: 60.9803, Test Loss: 162.7083\n",
      "Epoch [44/1000], Training Loss: 38.7066, Test Loss: 134.0158\n",
      "Epoch [45/1000], Training Loss: 29.9355, Test Loss: 88.3834\n",
      "Epoch [46/1000], Training Loss: 34.5662, Test Loss: 95.3775\n",
      "Epoch [47/1000], Training Loss: 17.0904, Test Loss: 60.2093\n",
      "Epoch [48/1000], Training Loss: 20.6179, Test Loss: 130.8976\n",
      "Epoch [49/1000], Training Loss: 16.6496, Test Loss: 113.2431\n",
      "Epoch [50/1000], Training Loss: 24.2811, Test Loss: 59.8845\n",
      "Epoch [51/1000], Training Loss: 25.3182, Test Loss: 88.9309\n",
      "Epoch [52/1000], Training Loss: 31.1138, Test Loss: 172.0784\n",
      "Epoch [53/1000], Training Loss: 29.2519, Test Loss: 104.8211\n",
      "Epoch [54/1000], Training Loss: 19.7917, Test Loss: 141.3569\n",
      "Epoch [55/1000], Training Loss: 17.8535, Test Loss: 179.7723\n",
      "Epoch [56/1000], Training Loss: 22.7631, Test Loss: 77.0795\n",
      "Epoch [57/1000], Training Loss: 13.3653, Test Loss: 91.8735\n",
      "Epoch [58/1000], Training Loss: 9.9223, Test Loss: 112.5595\n",
      "Epoch [59/1000], Training Loss: 17.4055, Test Loss: 68.9409\n",
      "Epoch [60/1000], Training Loss: 15.8364, Test Loss: 48.7843\n",
      "Epoch [61/1000], Training Loss: 23.3079, Test Loss: 169.9522\n",
      "Epoch [62/1000], Training Loss: 18.8846, Test Loss: 66.3678\n",
      "Epoch [63/1000], Training Loss: 20.7782, Test Loss: 167.5755\n",
      "Epoch [64/1000], Training Loss: 16.4794, Test Loss: 99.2773\n",
      "Epoch [65/1000], Training Loss: 10.7036, Test Loss: 86.8812\n",
      "Epoch [66/1000], Training Loss: 13.8558, Test Loss: 126.9904\n",
      "Epoch [67/1000], Training Loss: 15.4441, Test Loss: 54.2126\n",
      "Epoch [68/1000], Training Loss: 15.2065, Test Loss: 104.3018\n",
      "Epoch [69/1000], Training Loss: 15.8355, Test Loss: 109.6029\n",
      "Epoch [70/1000], Training Loss: 19.5755, Test Loss: 222.9875\n",
      "Epoch [71/1000], Training Loss: 16.6703, Test Loss: 119.8470\n",
      "Epoch [72/1000], Training Loss: 16.6673, Test Loss: 74.7996\n",
      "Epoch [73/1000], Training Loss: 16.8691, Test Loss: 95.2136\n",
      "Epoch [74/1000], Training Loss: 13.8700, Test Loss: 75.3580\n",
      "Epoch [75/1000], Training Loss: 16.6446, Test Loss: 108.3305\n",
      "Epoch [76/1000], Training Loss: 18.5490, Test Loss: 56.7682\n",
      "Epoch [77/1000], Training Loss: 8.2278, Test Loss: 127.3271\n",
      "Epoch [78/1000], Training Loss: 9.8370, Test Loss: 113.3524\n",
      "Epoch [79/1000], Training Loss: 18.5909, Test Loss: 104.7100\n",
      "Epoch [80/1000], Training Loss: 13.6550, Test Loss: 106.7812\n",
      "Epoch [81/1000], Training Loss: 14.1457, Test Loss: 97.5170\n",
      "Epoch [82/1000], Training Loss: 18.1288, Test Loss: 63.0150\n",
      "Epoch [83/1000], Training Loss: 9.6546, Test Loss: 127.3812\n",
      "Epoch [84/1000], Training Loss: 11.8396, Test Loss: 79.1720\n",
      "Epoch [85/1000], Training Loss: 10.7932, Test Loss: 55.0710\n",
      "Epoch [86/1000], Training Loss: 12.8294, Test Loss: 102.9883\n",
      "Epoch [87/1000], Training Loss: 10.5789, Test Loss: 84.8399\n",
      "Epoch [88/1000], Training Loss: 12.0891, Test Loss: 88.5756\n",
      "Epoch [89/1000], Training Loss: 17.6624, Test Loss: 88.7768\n",
      "Epoch [90/1000], Training Loss: 9.8802, Test Loss: 97.3398\n",
      "Epoch [91/1000], Training Loss: 12.9138, Test Loss: 69.1263\n",
      "Epoch [92/1000], Training Loss: 12.1583, Test Loss: 86.3713\n",
      "Epoch [93/1000], Training Loss: 16.5691, Test Loss: 92.1782\n",
      "Epoch [94/1000], Training Loss: 15.1766, Test Loss: 103.1153\n",
      "Epoch [95/1000], Training Loss: 14.7015, Test Loss: 107.7700\n",
      "Epoch [96/1000], Training Loss: 12.2936, Test Loss: 64.8222\n",
      "Epoch [97/1000], Training Loss: 14.6626, Test Loss: 24.3123\n",
      "Epoch [98/1000], Training Loss: 12.1784, Test Loss: 105.1399\n",
      "Epoch [99/1000], Training Loss: 12.7367, Test Loss: 129.7275\n",
      "Epoch [100/1000], Training Loss: 11.1861, Test Loss: 68.8822\n",
      "Epoch [101/1000], Training Loss: 11.0531, Test Loss: 97.3081\n",
      "Epoch [102/1000], Training Loss: 7.4553, Test Loss: 96.4151\n",
      "Epoch [103/1000], Training Loss: 6.7309, Test Loss: 105.6235\n",
      "Epoch [104/1000], Training Loss: 7.8346, Test Loss: 87.2253\n",
      "Epoch [105/1000], Training Loss: 7.4822, Test Loss: 45.1724\n",
      "Epoch [106/1000], Training Loss: 5.8050, Test Loss: 102.9276\n",
      "Epoch [107/1000], Training Loss: 5.7463, Test Loss: 87.1473\n",
      "Epoch [108/1000], Training Loss: 5.1458, Test Loss: 99.3032\n",
      "Epoch [109/1000], Training Loss: 7.4472, Test Loss: 92.0612\n",
      "Epoch [110/1000], Training Loss: 9.5067, Test Loss: 79.0040\n",
      "Epoch [111/1000], Training Loss: 4.8835, Test Loss: 70.9640\n",
      "Epoch [112/1000], Training Loss: 4.7133, Test Loss: 70.5322\n",
      "Epoch [113/1000], Training Loss: 5.0224, Test Loss: 74.1175\n",
      "Epoch [114/1000], Training Loss: 4.1417, Test Loss: 85.9612\n",
      "Epoch [115/1000], Training Loss: 4.7913, Test Loss: 77.9807\n",
      "Epoch [116/1000], Training Loss: 4.4563, Test Loss: 83.3043\n",
      "Epoch [117/1000], Training Loss: 4.8887, Test Loss: 110.1475\n",
      "Epoch [118/1000], Training Loss: 6.4257, Test Loss: 66.6378\n",
      "Epoch [119/1000], Training Loss: 4.2825, Test Loss: 99.0898\n",
      "Epoch [120/1000], Training Loss: 5.1803, Test Loss: 65.8957\n",
      "Epoch [121/1000], Training Loss: 7.4244, Test Loss: 118.3742\n",
      "Epoch [122/1000], Training Loss: 6.5891, Test Loss: 71.1284\n",
      "Epoch [123/1000], Training Loss: 4.7147, Test Loss: 70.7295\n",
      "Epoch [124/1000], Training Loss: 7.0470, Test Loss: 81.0609\n",
      "Epoch [125/1000], Training Loss: 3.8758, Test Loss: 64.9676\n",
      "Epoch [126/1000], Training Loss: 4.7630, Test Loss: 90.3233\n",
      "Epoch [127/1000], Training Loss: 4.4379, Test Loss: 66.5252\n",
      "Epoch [128/1000], Training Loss: 4.4144, Test Loss: 74.4659\n",
      "Epoch [129/1000], Training Loss: 4.4013, Test Loss: 78.3944\n",
      "Epoch [130/1000], Training Loss: 3.7388, Test Loss: 83.1875\n",
      "Epoch [131/1000], Training Loss: 3.0672, Test Loss: 63.9277\n",
      "Epoch [132/1000], Training Loss: 4.6316, Test Loss: 83.4513\n",
      "Epoch [133/1000], Training Loss: 5.3900, Test Loss: 117.4069\n",
      "Epoch [134/1000], Training Loss: 4.4673, Test Loss: 77.4411\n",
      "Epoch [135/1000], Training Loss: 4.3747, Test Loss: 63.2816\n",
      "Epoch [136/1000], Training Loss: 4.4171, Test Loss: 94.8970\n",
      "Epoch [137/1000], Training Loss: 3.8115, Test Loss: 106.1395\n",
      "Epoch [138/1000], Training Loss: 4.2713, Test Loss: 72.7249\n",
      "Epoch [139/1000], Training Loss: 4.1626, Test Loss: 51.2854\n",
      "Epoch [140/1000], Training Loss: 5.6529, Test Loss: 81.9005\n",
      "Epoch [141/1000], Training Loss: 4.4469, Test Loss: 74.3795\n",
      "Epoch [142/1000], Training Loss: 3.8082, Test Loss: 106.0888\n",
      "Epoch [143/1000], Training Loss: 7.0486, Test Loss: 52.6099\n",
      "Epoch [144/1000], Training Loss: 7.2416, Test Loss: 65.4239\n",
      "Epoch [145/1000], Training Loss: 4.3704, Test Loss: 26.5295\n",
      "Epoch [146/1000], Training Loss: 4.6195, Test Loss: 54.8917\n",
      "Epoch [147/1000], Training Loss: 5.8267, Test Loss: 82.8265\n",
      "Epoch [148/1000], Training Loss: 3.5773, Test Loss: 75.4494\n",
      "Epoch [149/1000], Training Loss: 5.1641, Test Loss: 236.3627\n",
      "Epoch [150/1000], Training Loss: 2.9873, Test Loss: 62.5043\n",
      "Epoch [151/1000], Training Loss: 4.6445, Test Loss: 93.7991\n",
      "Epoch [152/1000], Training Loss: 4.1548, Test Loss: 70.0598\n",
      "Epoch [153/1000], Training Loss: 4.5546, Test Loss: 71.2223\n",
      "Epoch [154/1000], Training Loss: 3.7829, Test Loss: 94.0095\n",
      "Epoch [155/1000], Training Loss: 4.8040, Test Loss: 69.6781\n",
      "Epoch [156/1000], Training Loss: 3.9168, Test Loss: 85.6787\n",
      "Epoch [157/1000], Training Loss: 5.0121, Test Loss: 83.7587\n",
      "Epoch [158/1000], Training Loss: 3.7017, Test Loss: 59.9375\n",
      "Epoch [159/1000], Training Loss: 4.0404, Test Loss: 50.4681\n",
      "Epoch [160/1000], Training Loss: 5.5806, Test Loss: 75.2734\n",
      "Epoch [161/1000], Training Loss: 3.6266, Test Loss: 79.0173\n",
      "Epoch [162/1000], Training Loss: 4.9769, Test Loss: 88.1900\n",
      "Epoch [163/1000], Training Loss: 2.8955, Test Loss: 84.1515\n",
      "Epoch [164/1000], Training Loss: 5.2712, Test Loss: 70.0263\n",
      "Epoch [165/1000], Training Loss: 4.9260, Test Loss: 69.1705\n",
      "Epoch [166/1000], Training Loss: 3.5502, Test Loss: 72.9644\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:35\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# 用您的模型和数据初始化函数\n",
    "model = SimplifiedConvNet().to(device)\n",
    "\n",
    "epochs=1000\n",
    "learning_rate=0.01\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化器和学习率衰减\n",
    "l2_lambda = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)  # 每100个epoch学习率乘以0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch < 500:lambda_weight=0.000001\n",
    "    else :lambda_weight=0.00001\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs_3d, inputs_2d, inputs_1d, inputs_0d, targets in train_dataloader:\n",
    "        inputs_3d, inputs_2d, inputs_1d, inputs_0d, targets = inputs_3d.to(device), inputs_2d.to(device), inputs_1d.to(device), inputs_0d.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs_3d, inputs_2d, inputs_1d, inputs_0d).squeeze(1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        weight_reg = lambda_weight * (model.weight_2d.norm(2) + model.weight_3d.norm(2) + model.weight_1d.norm(2))\n",
    "        loss += weight_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "\n",
    "    # 测试阶段\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs_3d, inputs_2d, inputs_1d, inputs_0d, targets in test_dataloader:\n",
    "            inputs_3d, inputs_2d, inputs_1d, inputs_0d, targets = inputs_3d.to(device), inputs_2d.to(device), inputs_1d.to(device), inputs_0d.to(device), targets.to(device)\n",
    "            outputs = model(inputs_3d, inputs_2d, inputs_1d, inputs_0d).squeeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = running_test_loss / len(test_dataloader)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "\n",
    "print('Finished Training and Evaluation')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:40:58.336122100Z",
     "start_time": "2024-01-02T06:38:56.246435700Z"
    }
   },
   "id": "456f3aae99482396"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "70.15827178955078"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "for inputs_3d, inputs_2d, inputs_1d, inputs_0d, targets in train_dataloader:\n",
    "    inputs_3d, inputs_2d, inputs_1d, inputs_0d, targets = inputs_3d.to(device), inputs_2d.to(device), inputs_1d.to(device), inputs_0d.to(device), targets.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs_3d, inputs_2d, inputs_1d, inputs_0d).squeeze(1)\n",
    "    loss = criterion(outputs, targets)\n",
    "    weight_reg = lambda_weight * (model.weight_2d.norm(2) + model.weight_3d.norm(2) + model.weight_1d.norm(2))\n",
    "    loss += weight_reg\n",
    "    running_train_loss += loss.item()\n",
    "\n",
    "avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        \n",
    "running_test_loss/ len(train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:42:51.625797400Z",
     "start_time": "2024-01-02T06:42:51.364092600Z"
    }
   },
   "id": "46ecfe4c655a446e"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([27.0433, 15.2809, 28.0996], device='cuda:0')"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:41:09.722688500Z",
     "start_time": "2024-01-02T06:41:09.673201300Z"
    }
   },
   "id": "9805a189a4f1bf5"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([34.9976, 18.8765, 38.3754], device='cuda:0')"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T06:41:11.323139Z",
     "start_time": "2024-01-02T06:41:11.277139100Z"
    }
   },
   "id": "131fad2373ab905f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
