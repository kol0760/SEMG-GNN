{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 导入库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f6407a4ac46154"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 导入pytoch并检查是否为GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ca13523d345c476"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:06:02.592167700Z",
     "start_time": "2023-12-31T08:05:53.567028300Z"
    }
   },
   "id": "52bd2374e02b22f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 导入其它库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c359a129d47b5473"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:06:04.055475700Z",
     "start_time": "2023-12-31T08:06:02.597161500Z"
    }
   },
   "id": "776639e04f4c2b12"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 定义文件名和将要存储数据的变量名\n",
    "files_to_load = {\n",
    "    'total_elec.pkl': 'total_elec',     #3维\n",
    "    'total_spms.pkl': 'total_spms',     #2维\n",
    "    'total_chg.pkl': 'total_chg',      #1维\n",
    "    'total_vfukui.pkl': 'total_vfukui',   #1维\n",
    "    'total_Pop.pkl': 'total_Pop',      #1维\n",
    "    'total_Mol.pkl': 'total_Mol',      #0维\n",
    "    'total_IP_EA.pkl': 'total_IP_EA',    #0维\n",
    "\n",
    "\n",
    "    'total_out_basics_final.pkl': 'total_out_basics_final', #索引\n",
    "    'total_energy.pkl': 'total_energy'  #回归值输出\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# 创建一个字典来存储加载的数据\n",
    "loaded_data = {}\n",
    "\n",
    "# 循环遍历文件并加载数据\n",
    "for file_name, data_name in files_to_load.items():\n",
    "    with open(\"out_data/\"+file_name, 'rb') as file:\n",
    "        loaded_data[data_name] = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:08:18.054601700Z",
     "start_time": "2023-12-31T08:08:17.814043200Z"
    }
   },
   "id": "c2b1eb148360ede2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "total_energy=np.array(loaded_data['total_energy']).astype(np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:08:55.002366800Z",
     "start_time": "2023-12-31T08:08:54.975367400Z"
    }
   },
   "id": "eafddd0e78d475b9"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def categorize_energy(energy):\n",
    "    if energy < 30:\n",
    "        return 30\n",
    "    elif energy >= 40:\n",
    "        return 40\n",
    "    else:\n",
    "        return energy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:01:41.619491300Z",
     "start_time": "2023-12-31T10:01:41.546497100Z"
    }
   },
   "id": "303a77f136133f71"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "array([36.14713669, 39.33295441, 40.        , 39.11709595, 38.07544327,\n       40.        , 34.6210556 , 35.86162567, 35.35962677, 35.11991882,\n       36.25381088, 37.9091568 , 40.        , 30.8660965 , 37.9693985 ,\n       40.        , 40.        , 39.95355225, 39.28463745, 33.1865921 ,\n       34.67816162, 37.54771805, 37.31616974, 30.        , 30.61446953,\n       30.        , 30.        , 39.19678879, 36.54748154, 36.1973381 ,\n       30.79456139, 37.36950684, 40.        , 37.766716  , 36.0442276 ,\n       35.07536697, 37.76797104, 34.09082031, 35.77314758, 33.92076874,\n       34.89715576, 33.98665619, 36.41570663, 35.11364365, 33.92641449,\n       31.6812191 , 35.5924263 , 35.79008865, 38.22541809, 35.76499176,\n       30.43249512, 35.11238861, 33.91700363, 31.68310165, 35.57046509,\n       35.79448318, 38.22541809, 35.75369644, 30.42810249, 34.0832901 ,\n       37.88594055, 34.81746674, 36.66419601, 33.58882141, 36.44080734,\n       37.0940361 , 32.16753387, 31.74648094, 35.31570053, 34.585289  ,\n       38.45257187, 37.15176392, 37.53955841, 39.97363281, 33.04917145,\n       30.        , 36.66043091, 33.58693695, 36.44206238, 37.09340668,\n       32.17004013, 31.74585152, 35.33013153, 34.85260391, 38.45069122,\n       37.53642273, 39.9836731 , 33.05419159, 38.76945877, 34.54011154,\n       33.74694824, 39.36558533, 38.66654968, 39.26016617, 36.37429047,\n       35.83966064, 35.65768814, 35.95888519, 34.3612709 , 32.37774658,\n       30.42873001, 33.24494934, 31.0085392 , 33.182827  , 33.82915115,\n       37.56340408, 30.96084976, 40.        , 33.56121063, 40.        ,\n       40.        , 38.37287903, 38.7098465 , 38.01269531, 37.1116066 ,\n       38.65086365, 36.05050278, 39.97927856, 32.59423065, 37.83385849,\n       30.        , 38.85668182, 38.22980881, 33.39115524, 39.09199524,\n       38.52410889, 36.91896057, 33.66223526, 40.        , 33.50410843,\n       38.90499878, 35.87103653, 37.7416153 , 37.70898438, 33.92076874,\n       38.68349075, 40.        , 31.32731056, 31.43649483, 35.37719345,\n       36.47908401, 40.        , 35.87605667, 38.4111557 , 39.1258812 ,\n       40.        , 40.        , 40.        , 31.84876251, 37.64874649,\n       40.        , 34.60913467, 40.        , 39.21247482, 37.49249649,\n       39.13529205, 34.45539856, 30.8058567 , 34.51689148, 30.39609909,\n       31.95669174, 30.        , 36.82923126, 40.        , 35.2222023 ,\n       38.93888474, 36.03293228, 32.21585083, 37.41468811, 31.54379654,\n       31.53815079, 40.        , 31.88202095, 40.        , 31.54379654,\n       32.15937424, 31.68059158, 34.89904022, 35.71604538, 39.89707565,\n       34.97120285, 30.        , 32.61180115, 30.        , 30.        ,\n       30.        , 30.        , 40.        , 38.09615326, 37.56340408,\n       40.        , 40.        , 37.2189064 , 31.62348938, 35.28871536,\n       40.        , 37.9091568 , 36.39751053, 31.87951088, 35.59682083,\n       31.03991508, 40.        , 32.45743561, 30.        , 30.        ,\n       30.        , 30.        , 30.        , 34.22510529, 31.8412323 ,\n       33.97661591, 35.30315018, 30.        , 30.        , 32.99708557,\n       34.63360596, 30.        , 39.24636078, 37.53077316, 36.51548004,\n       36.22494888, 30.15702248, 38.45194626, 35.46190643, 30.        ,\n       34.97873306, 34.62921524, 38.51155853, 37.80499268, 30.        ,\n       35.88672638, 30.        , 33.99606705, 36.38496017, 31.9585743 ,\n       30.        , 36.51673508, 31.93159294, 39.64294052, 38.3753891 ,\n       35.13246918, 35.63384247, 40.        , 38.80397034, 40.        ,\n       35.81205368, 31.65674782, 40.        , 40.        , 30.        ,\n       36.58952332, 37.74977112, 34.89966583, 32.60741043, 35.23412323,\n       40.        , 33.18784714, 37.23522186, 39.33922958, 33.60576248,\n       33.12446976, 30.        , 34.99755859, 35.71165085, 30.87613678,\n       30.39421654, 38.99912643, 34.26903152, 34.70074844, 36.7671051 ,\n       37.60419083, 39.01355743, 30.        , 37.62678146, 39.95041656,\n       38.71173096, 35.20024109, 37.38331223, 36.92210007, 37.38394165,\n       36.86436844, 31.07568169, 38.54669571, 38.22165298, 40.        ,\n       40.        , 40.        , 30.        , 30.        , 30.        ,\n       30.        ])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorized_energy = np.array([categorize_energy(e) for e in total_energy])\n",
    "categorized_energy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:01:43.786504400Z",
     "start_time": "2023-12-31T10:01:43.724507200Z"
    }
   },
   "id": "8cd2e42cde6bd096"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each category in categorized_energy\n",
    "categories, counts = np.unique(categorized_energy, return_counts=True)\n",
    "\n",
    "# Create a bar plot to show the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, color='blue')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Distribution of Categorized Energy Values')\n",
    "plt.xticks(categories)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:01:49.696912700Z",
     "start_time": "2023-12-31T10:01:47.752865800Z"
    }
   },
   "id": "b2838142162c4756"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([36.147137, 39.332954, 40.407864, 39.117096, 38.075443, 40.46308 ,\n       34.621056, 35.861626, 35.359627, 35.11992 , 36.25381 , 37.909157,\n       41.393036, 30.866096, 37.9694  , 42.34621 , 42.252712, 39.953552,\n       39.284637, 33.186592, 34.67816 , 37.547718, 37.31617 , 29.905396,\n       30.61447 , 29.382687, 29.04572 , 39.19679 , 36.54748 , 36.19734 ,\n       30.794561, 37.369507, 44.98422 , 37.766716, 36.044228, 35.075367,\n       37.76797 , 34.09082 , 35.773148, 33.92077 , 34.897156, 33.986656,\n       36.415707, 35.113644, 33.926414, 31.68122 , 35.592426, 35.79009 ,\n       38.225418, 35.76499 , 30.432495, 35.11239 , 33.917004, 31.683102,\n       35.570465, 35.794483, 38.225418, 35.753696, 30.428102, 34.08329 ,\n       37.88594 , 34.817467, 36.664196, 33.58882 , 36.440807, 37.094036,\n       32.167534, 31.74648 , 35.3157  , 34.58529 , 38.45257 , 37.151764,\n       37.53956 , 39.973633, 33.04917 , 28.226206, 36.66043 , 33.586937,\n       36.442062, 37.093407, 32.17004 , 31.745852, 35.33013 , 34.852604,\n       38.45069 , 37.536423, 39.983673, 33.05419 , 38.76946 , 34.54011 ,\n       33.74695 , 39.365585, 38.66655 , 39.260166, 36.37429 , 35.83966 ,\n       35.65769 , 35.958885, 34.36127 , 32.377747, 30.42873 , 33.24495 ,\n       31.00854 , 33.182827, 33.82915 , 37.563404, 30.96085 , 42.26589 ,\n       33.56121 , 41.852997, 46.473278, 38.37288 , 38.709846, 38.012695,\n       37.111607, 38.650864, 36.050503, 39.97928 , 32.59423 , 37.83386 ,\n       27.359627, 38.85668 , 38.22981 , 33.391155, 39.091995, 38.52411 ,\n       36.91896 , 33.662235, 40.141174, 33.50411 , 38.905   , 35.871037,\n       37.741615, 37.708984, 33.92077 , 38.68349 , 40.66702 , 31.32731 ,\n       31.436495, 35.377193, 36.479084, 44.44708 , 35.876057, 38.411156,\n       39.12588 , 40.49634 , 41.135136, 41.67541 , 31.848763, 37.648746,\n       41.800285, 34.609135, 40.213337, 39.212475, 37.492496, 39.135292,\n       34.4554  , 30.805857, 34.51689 , 30.3961  , 31.956692, 27.402296,\n       36.82923 , 44.023518, 35.222202, 38.938885, 36.032932, 32.21585 ,\n       37.41469 , 31.543797, 31.53815 , 42.5194  , 31.88202 , 41.708042,\n       31.543797, 32.159374, 31.680592, 34.89904 , 35.716045, 39.897076,\n       34.971203, 27.064701, 32.6118  , 24.755503, 23.74962 , 24.20895 ,\n       26.67126 , 43.225338, 38.096153, 37.563404, 46.26181 , 40.27358 ,\n       37.218906, 31.62349 , 35.288715, 40.23028 , 37.909157, 36.39751 ,\n       31.87951 , 35.59682 , 31.039915, 40.86029 , 32.457436, 18.876455,\n       28.542465, 26.44536 , 27.585527, 27.754326, 34.225105, 31.841232,\n       33.976616, 35.30315 , 27.747421, 28.166592, 32.997086, 34.633606,\n       24.94689 , 39.24636 , 37.530773, 36.51548 , 36.22495 , 30.157022,\n       38.451946, 35.461906, 29.014345, 34.978733, 34.629215, 38.51156 ,\n       37.804993, 27.714165, 35.886726, 28.125805, 33.996067, 36.38496 ,\n       31.958574, 28.629686, 36.516735, 31.931593, 39.64294 , 38.37539 ,\n       35.13247 , 35.633842, 41.463943, 38.80397 , 40.562855, 35.812054,\n       31.656748, 40.901707, 41.6911  , 29.684515, 36.589523, 37.74977 ,\n       34.899666, 32.60741 , 35.234123, 40.38088 , 33.187847, 37.23522 ,\n       39.33923 , 33.605762, 33.12447 , 26.901552, 34.99756 , 35.71165 ,\n       30.876137, 30.394217, 38.999126, 34.26903 , 34.70075 , 36.767105,\n       37.60419 , 39.013557, 29.293583, 37.62678 , 39.950417, 38.71173 ,\n       35.20024 , 37.383312, 36.9221  , 37.38394 , 36.86437 , 31.075682,\n       38.546696, 38.221653, 46.307617, 47.20055 , 42.327385, 21.138592,\n       22.822802, 22.82343 , 23.20746 ], dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_energy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:08:56.627311800Z",
     "start_time": "2023-12-31T08:08:56.580313100Z"
    }
   },
   "id": "19d85d276332959a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据准备并清洗"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87afc77f53b43fd9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2d & 3d 的准备\n",
    "2d为spms，形状为10*20\n",
    "3d为elec，形状为7*7*7"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc9ea5d7cc0a41c"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 6, 7, 7, 7) (291, 6, 10, 20)\n"
     ]
    }
   ],
   "source": [
    "elec = []\n",
    "spms = []\n",
    "\n",
    "for i in range(len(loaded_data['total_out_basics_final'])):\n",
    "    indices = [index - 1 for index in loaded_data['total_out_basics_final'][i][0]]\n",
    "    elec_values = [loaded_data['total_elec'][i][index] for index in indices]\n",
    "    elec.append(elec_values)\n",
    "\n",
    "    spms_values = [loaded_data['total_spms'][i][index] for index in indices]\n",
    "    spms.append(spms_values)\n",
    "elec = np.array(elec)\n",
    "spms = np.array(spms)\n",
    "print(elec.shape,spms.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:01:57.285601500Z",
     "start_time": "2023-12-31T10:01:57.228602100Z"
    }
   },
   "id": "d53274edfd515099"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1d 的准备\n",
    "由电荷，福井函数等组成，在列的方向上进行了归一化"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50ed61a5b3f52e4e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 6) (291, 6, 3) (291, 6, 4)\n",
      "(291, 6, 4)\n",
      "(291, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "chg = []\n",
    "vfukui = []\n",
    "Pop = []\n",
    "\n",
    "for i in range(len(loaded_data['total_out_basics_final'])):\n",
    "    indices = [index - 1 for index in loaded_data['total_out_basics_final'][i][0]]\n",
    "    chg_values = [loaded_data['total_chg'][i][index] for index in indices]\n",
    "    chg.append(chg_values)\n",
    "\n",
    "    vfukui_values = [loaded_data['total_vfukui'][i][index] for index in indices]\n",
    "    vfukui.append(vfukui_values)\n",
    "\n",
    "    Pop_values = [loaded_data['total_Pop'][i][index] for index in indices]\n",
    "    Pop.append(Pop_values)\n",
    "chg = np.array(chg)\n",
    "vfukui = np.array(vfukui)[:, :, 1:]\n",
    "Pop=np.array(Pop)[:, :, 3:]\n",
    "print(chg.shape,vfukui.shape,Pop.shape)\n",
    "resule_x_1d = np.concatenate((chg[:, :, np.newaxis], vfukui), axis=2).astype(np.float32)\n",
    "print(resule_x_1d.shape)\n",
    "normalized_result = np.empty_like(resule_x_1d)\n",
    "for i in range(resule_x_1d.shape[2]):\n",
    "    slice_i = resule_x_1d[:, :, i]\n",
    "    min_val = np.min(slice_i)\n",
    "    max_val = np.max(slice_i)\n",
    "    if min_val != max_val:\n",
    "        normalized_slice = 2 * ((slice_i - min_val) / (max_val - min_val)) - 1\n",
    "    else:\n",
    "        normalized_slice = np.zeros_like(slice_i)\n",
    "    normalized_result[:, :, i] = normalized_slice\n",
    "print(normalized_result.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:31:28.014027800Z",
     "start_time": "2023-12-31T08:31:27.942030400Z"
    }
   },
   "id": "1ac9975259c3242b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0d数据\n",
    "分子整体的数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a0c1b9747a2eb79"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "IP_EA=np.array(loaded_data['total_IP_EA'])\n",
    "Mol=np.array(loaded_data['total_Mol'])\n",
    "IP_EA=IP_EA * 627.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:31:30.317346500Z",
     "start_time": "2023-12-31T08:31:30.287338500Z"
    }
   },
   "id": "ad22f6e1d9e0a990"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 神经网络部分"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "626a3e84621946ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义数据集和数据库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a115e3378ddac5b"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(291, 6, 4)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader ,random_split\n",
    "\n",
    "class ComplexDataset(Dataset):\n",
    "    def __init__(self, elec, spms, normalized_result, energy):\n",
    "        self.elec = torch.tensor(elec, dtype=torch.float32)  # 将elec转换为张量\n",
    "        self.spms = torch.tensor(spms, dtype=torch.float32)  # 将spms转换为张量\n",
    "        self.normalized_result = torch.tensor(normalized_result, dtype=torch.float32)  # 将additional_data转换为张量\n",
    "        self.energy = torch.tensor(energy, dtype=torch.float32)  # 将energy转换为张量\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energy)  # 返回样本总数\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引返回单个样本\n",
    "        return self.spms[idx], self.elec[idx], self.normalized_result[idx], self.energy[idx]\n",
    "\n",
    "# 创建ComplexDataset实例\n",
    "dataset = ComplexDataset(elec, spms, normalized_result, categorized_energy)\n",
    "normalized_result.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:02:12.751911900Z",
     "start_time": "2023-12-31T10:02:12.687019900Z"
    }
   },
   "id": "4bb44aca48e25c5"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# 随机划分数据集\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 在创建DataLoader时使用\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:02:15.242734900Z",
     "start_time": "2023-12-31T10:02:15.222735100Z"
    }
   },
   "id": "49243e20e45b3dd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义神经网络"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "170ec582aa42e96c"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(query_dim, value_dim)\n",
    "        self.key_layer = nn.Linear(key_dim, value_dim)\n",
    "        self.value_layer = nn.Linear(key_dim, value_dim)\n",
    "    \n",
    "    def forward(self, query, keys, values):\n",
    "        query = self.query_layer(query)  # [Batch, Query_Dim]\n",
    "        keys = self.key_layer(keys)      # [Batch, Key_Dim]\n",
    "        values = self.value_layer(values)# [Batch, Value_Dim]\n",
    "        \n",
    "        # Reshape for bmm\n",
    "        query = query.unsqueeze(2)  # [Batch, Query_Dim, 1]\n",
    "        keys = keys.unsqueeze(1)    # [Batch, 1, Key_Dim]\n",
    "        \n",
    "        # Compute attention scores and apply to values\n",
    "        # After unsqueeze: query is [Batch, Query_Dim, 1], keys is [Batch, 1, Key_Dim]\n",
    "        attention_scores = F.softmax(torch.bmm(keys, query), dim=-1)  # [Batch, 1, 1]\n",
    "        \n",
    "        # Expand and apply scores to values\n",
    "        values = values.unsqueeze(1)  # [Batch, 1, Value_Dim]\n",
    "        attended_values = attention_scores * values  # Element-wise multiplication\n",
    "        attended_values = attended_values.squeeze(1)  # Removing the singleton dimension\n",
    "        \n",
    "        return attended_values\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:02:18.974246500Z",
     "start_time": "2023-12-31T10:02:18.948280100Z"
    }
   },
   "id": "e4b68ed562191ac3"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class SimplifiedConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedConvNet, self).__init__()\n",
    "        # 2D数据的卷积层\n",
    "        self.conv2d_1 = nn.Conv2d(6, 32, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn2d_1 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2d_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(32, 64, kernel_size=1, stride=1, padding=1)\n",
    "        self.bn2d_2 = nn.BatchNorm2d(64)\n",
    "        self.maxpool2d_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 3D数据的卷积层\n",
    "        self.conv3d_1 = nn.Conv3d(6, 32, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn3d_1 = nn.BatchNorm3d(32)\n",
    "        self.maxpool3d_1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3d_2 = nn.Conv3d(32, 64, kernel_size=1, stride=1, padding=1)\n",
    "        self.bn3d_2 = nn.BatchNorm3d(64)\n",
    "        self.maxpool3d_2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc_1d1 = nn.Linear(6 * 4, 128)\n",
    "        self.fc_1d2 = nn.Linear(128, 256)\n",
    "        self.bn1d_1 = nn.BatchNorm1d(256)\n",
    "        self.dropout2d = nn.Dropout(p=0.4)\n",
    "        self.dropout3d = nn.Dropout(p=0.5)\n",
    "        self.dropout1d = nn.Dropout(p=0.05)   \n",
    "        self.dropout = nn.Dropout(p=0.3)   \n",
    "        \n",
    "        self.weight_2d = nn.Parameter(torch.ones(1))\n",
    "        self.weight_3d = nn.Parameter(torch.ones(1))\n",
    "        self.weight_1d = nn.Parameter(torch.ones(1))\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(1152+1728+256, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128,64)\n",
    "        self.fc6 = nn.Linear(64,1)\n",
    "        \n",
    "        self.attention_2d_to_3d = CrossModalAttention(query_dim=1152, key_dim=1728, value_dim=1728)\n",
    "        self.attention_3d_to_2d = CrossModalAttention(query_dim=1728, key_dim=1152, value_dim=1152)\n",
    "\n",
    "    def forward(self, x2d, x3d, x1d):\n",
    "        # 处理2D数据\n",
    "        x2d = F.relu(self.bn2d_1(self.conv2d_1(x2d)))\n",
    "        x2d = self.maxpool2d_1(x2d)\n",
    "        x2d = F.relu(self.bn2d_2(self.conv2d_2(x2d)))\n",
    "        x2d = self.maxpool2d_2(x2d)\n",
    "        x2d = x2d.flatten(1)\n",
    "        # 处理3D数据\n",
    "        x3d = F.relu(self.bn3d_1(self.conv3d_1(x3d)))\n",
    "        x3d = self.maxpool3d_1(x3d)\n",
    "        x3d = F.relu(self.bn3d_2(self.conv3d_2(x3d)))\n",
    "        x3d = self.maxpool3d_2(x3d)\n",
    "        x3d = x3d.flatten(1)\n",
    "\n",
    "        x1d = x1d.view(x1d.size(0),-1)\n",
    "        x1d = F.relu(self.fc_1d1(x1d))\n",
    "        x1d = F.relu(self.fc_1d2(x1d))\n",
    "        x1d = self.bn1d_1(x1d)\n",
    "\n",
    "        \n",
    "        attended_3d = self.attention_2d_to_3d(x2d, x3d, x3d)\n",
    "        attended_2d = self.attention_3d_to_2d(x3d, x2d, x2d)\n",
    "        \n",
    "        x2d = self.dropout2d(attended_2d)\n",
    "        x3d = self.dropout3d(attended_3d)\n",
    "        x1d = self.dropout1d(x1d)          \n",
    "        \n",
    "        weighted_2d = self.weight_2d * x2d\n",
    "        weighted_3d = self.weight_3d * x3d\n",
    "        weighted_1d = self.weight_1d * x1d\n",
    "        \n",
    "        combined = torch.cat((weighted_2d, weighted_3d, weighted_1d), dim=1)\n",
    "\n",
    "    \n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:02:33.724622600Z",
     "start_time": "2023-12-31T10:02:33.679626200Z"
    }
   },
   "id": "6a453134ea58789d"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 1542.4643, Test Loss: 84.8309\n",
      "Epoch [2/1000], Training Loss: 123.4714, Test Loss: 29.0368\n",
      "Epoch [3/1000], Training Loss: 103.5761, Test Loss: 28.5275\n",
      "Epoch [4/1000], Training Loss: 64.9659, Test Loss: 24.7681\n",
      "Epoch [5/1000], Training Loss: 60.5825, Test Loss: 13.5414\n",
      "Epoch [6/1000], Training Loss: 62.7275, Test Loss: 15.6456\n",
      "Epoch [7/1000], Training Loss: 65.4588, Test Loss: 80.5691\n",
      "Epoch [8/1000], Training Loss: 56.0571, Test Loss: 13.6701\n",
      "Epoch [9/1000], Training Loss: 41.4198, Test Loss: 79.9206\n",
      "Epoch [10/1000], Training Loss: 48.5374, Test Loss: 55.4369\n",
      "Epoch [11/1000], Training Loss: 36.6162, Test Loss: 21.2923\n",
      "Epoch [12/1000], Training Loss: 40.9177, Test Loss: 54.1730\n",
      "Epoch [13/1000], Training Loss: 43.8088, Test Loss: 33.0534\n",
      "Epoch [14/1000], Training Loss: 47.0345, Test Loss: 180.9216\n",
      "Epoch [15/1000], Training Loss: 33.3506, Test Loss: 61.4184\n",
      "Epoch [16/1000], Training Loss: 58.3660, Test Loss: 188.5971\n",
      "Epoch [17/1000], Training Loss: 41.3394, Test Loss: 105.5299\n",
      "Epoch [18/1000], Training Loss: 29.5575, Test Loss: 65.0723\n",
      "Epoch [19/1000], Training Loss: 24.3157, Test Loss: 169.2474\n",
      "Epoch [20/1000], Training Loss: 22.8621, Test Loss: 68.7285\n",
      "Epoch [21/1000], Training Loss: 20.8263, Test Loss: 96.5559\n",
      "Epoch [22/1000], Training Loss: 25.8076, Test Loss: 160.7579\n",
      "Epoch [23/1000], Training Loss: 25.3889, Test Loss: 48.2056\n",
      "Epoch [24/1000], Training Loss: 21.9659, Test Loss: 63.9925\n",
      "Epoch [25/1000], Training Loss: 52.9597, Test Loss: 290.5343\n",
      "Epoch [26/1000], Training Loss: 48.9327, Test Loss: 54.7312\n",
      "Epoch [27/1000], Training Loss: 59.7419, Test Loss: 126.0386\n",
      "Epoch [28/1000], Training Loss: 41.3642, Test Loss: 29.1612\n",
      "Epoch [29/1000], Training Loss: 37.9016, Test Loss: 55.1915\n",
      "Epoch [30/1000], Training Loss: 29.8126, Test Loss: 75.1810\n",
      "Epoch [31/1000], Training Loss: 14.1284, Test Loss: 155.1311\n",
      "Epoch [32/1000], Training Loss: 18.3832, Test Loss: 70.0007\n",
      "Epoch [33/1000], Training Loss: 16.6422, Test Loss: 80.1202\n",
      "Epoch [34/1000], Training Loss: 21.7123, Test Loss: 153.3253\n",
      "Epoch [35/1000], Training Loss: 19.0408, Test Loss: 57.9463\n",
      "Epoch [36/1000], Training Loss: 24.4202, Test Loss: 106.8091\n",
      "Epoch [37/1000], Training Loss: 34.5751, Test Loss: 112.4419\n",
      "Epoch [38/1000], Training Loss: 22.9358, Test Loss: 164.2279\n",
      "Epoch [39/1000], Training Loss: 26.4350, Test Loss: 43.9387\n",
      "Epoch [40/1000], Training Loss: 21.8076, Test Loss: 105.9350\n",
      "Epoch [41/1000], Training Loss: 22.6639, Test Loss: 90.1300\n",
      "Epoch [42/1000], Training Loss: 27.7086, Test Loss: 189.0530\n",
      "Epoch [43/1000], Training Loss: 27.4796, Test Loss: 116.1836\n",
      "Epoch [44/1000], Training Loss: 46.4983, Test Loss: 80.2464\n",
      "Epoch [45/1000], Training Loss: 57.1612, Test Loss: 17.9130\n",
      "Epoch [46/1000], Training Loss: 83.2257, Test Loss: 124.9935\n",
      "Epoch [47/1000], Training Loss: 62.0368, Test Loss: 43.3728\n",
      "Epoch [48/1000], Training Loss: 45.8116, Test Loss: 94.8531\n",
      "Epoch [49/1000], Training Loss: 23.9017, Test Loss: 92.9055\n",
      "Epoch [50/1000], Training Loss: 32.8796, Test Loss: 102.5000\n",
      "Epoch [51/1000], Training Loss: 29.1836, Test Loss: 79.4521\n",
      "Epoch [52/1000], Training Loss: 29.7528, Test Loss: 90.5597\n",
      "Epoch [53/1000], Training Loss: 15.1526, Test Loss: 124.5012\n",
      "Epoch [54/1000], Training Loss: 20.0227, Test Loss: 63.3188\n",
      "Epoch [55/1000], Training Loss: 17.7246, Test Loss: 114.8491\n",
      "Epoch [56/1000], Training Loss: 20.6219, Test Loss: 42.4794\n",
      "Epoch [57/1000], Training Loss: 24.4433, Test Loss: 89.5351\n",
      "Epoch [58/1000], Training Loss: 22.7930, Test Loss: 49.9313\n",
      "Epoch [59/1000], Training Loss: 28.4166, Test Loss: 54.6067\n",
      "Epoch [60/1000], Training Loss: 13.0434, Test Loss: 59.1552\n",
      "Epoch [61/1000], Training Loss: 14.1507, Test Loss: 54.9284\n",
      "Epoch [62/1000], Training Loss: 12.7459, Test Loss: 63.8248\n",
      "Epoch [63/1000], Training Loss: 15.1185, Test Loss: 71.2455\n",
      "Epoch [64/1000], Training Loss: 12.0898, Test Loss: 127.9942\n",
      "Epoch [65/1000], Training Loss: 12.2250, Test Loss: 49.6926\n",
      "Epoch [66/1000], Training Loss: 14.5496, Test Loss: 157.1341\n",
      "Epoch [67/1000], Training Loss: 23.1724, Test Loss: 69.9898\n",
      "Epoch [68/1000], Training Loss: 15.0068, Test Loss: 163.5882\n",
      "Epoch [69/1000], Training Loss: 18.0888, Test Loss: 54.9131\n",
      "Epoch [70/1000], Training Loss: 15.9244, Test Loss: 86.6777\n",
      "Epoch [71/1000], Training Loss: 11.0204, Test Loss: 71.6803\n",
      "Epoch [72/1000], Training Loss: 12.3612, Test Loss: 53.9066\n",
      "Epoch [73/1000], Training Loss: 15.1119, Test Loss: 63.3106\n",
      "Epoch [74/1000], Training Loss: 19.8458, Test Loss: 76.2787\n",
      "Epoch [75/1000], Training Loss: 15.1997, Test Loss: 89.0512\n",
      "Epoch [76/1000], Training Loss: 10.2888, Test Loss: 56.1591\n",
      "Epoch [77/1000], Training Loss: 11.2865, Test Loss: 55.8213\n",
      "Epoch [78/1000], Training Loss: 15.3067, Test Loss: 87.1453\n",
      "Epoch [79/1000], Training Loss: 12.3070, Test Loss: 37.8180\n",
      "Epoch [80/1000], Training Loss: 16.5077, Test Loss: 86.6877\n",
      "Epoch [81/1000], Training Loss: 11.1439, Test Loss: 77.4811\n",
      "Epoch [82/1000], Training Loss: 8.7383, Test Loss: 73.7831\n",
      "Epoch [83/1000], Training Loss: 9.0603, Test Loss: 82.4603\n",
      "Epoch [84/1000], Training Loss: 12.4282, Test Loss: 102.2621\n",
      "Epoch [85/1000], Training Loss: 10.5919, Test Loss: 44.5136\n",
      "Epoch [86/1000], Training Loss: 9.7749, Test Loss: 88.4162\n",
      "Epoch [87/1000], Training Loss: 9.4315, Test Loss: 86.3318\n",
      "Epoch [88/1000], Training Loss: 10.7066, Test Loss: 72.3429\n",
      "Epoch [89/1000], Training Loss: 9.3760, Test Loss: 59.6221\n",
      "Epoch [90/1000], Training Loss: 12.6562, Test Loss: 86.1218\n",
      "Epoch [91/1000], Training Loss: 8.8152, Test Loss: 62.5412\n",
      "Epoch [92/1000], Training Loss: 6.3068, Test Loss: 73.7103\n",
      "Epoch [93/1000], Training Loss: 10.0365, Test Loss: 85.8432\n",
      "Epoch [94/1000], Training Loss: 16.0586, Test Loss: 46.0167\n",
      "Epoch [95/1000], Training Loss: 12.1864, Test Loss: 93.5965\n",
      "Epoch [96/1000], Training Loss: 7.5475, Test Loss: 61.7611\n",
      "Epoch [97/1000], Training Loss: 6.5981, Test Loss: 75.6176\n",
      "Epoch [98/1000], Training Loss: 12.5441, Test Loss: 48.8659\n",
      "Epoch [99/1000], Training Loss: 9.1124, Test Loss: 52.2639\n",
      "Epoch [100/1000], Training Loss: 7.8833, Test Loss: 84.0142\n",
      "Epoch [101/1000], Training Loss: 8.8837, Test Loss: 62.6906\n",
      "Epoch [102/1000], Training Loss: 5.3351, Test Loss: 68.2673\n",
      "Epoch [103/1000], Training Loss: 5.3573, Test Loss: 65.2096\n",
      "Epoch [104/1000], Training Loss: 3.5198, Test Loss: 78.7579\n",
      "Epoch [105/1000], Training Loss: 5.7926, Test Loss: 69.1571\n",
      "Epoch [106/1000], Training Loss: 4.4464, Test Loss: 71.1291\n",
      "Epoch [107/1000], Training Loss: 4.0068, Test Loss: 72.9175\n",
      "Epoch [108/1000], Training Loss: 4.2405, Test Loss: 74.7225\n",
      "Epoch [109/1000], Training Loss: 3.1302, Test Loss: 73.1210\n",
      "Epoch [110/1000], Training Loss: 3.9623, Test Loss: 71.5697\n",
      "Epoch [111/1000], Training Loss: 3.6305, Test Loss: 68.3421\n",
      "Epoch [112/1000], Training Loss: 2.5907, Test Loss: 65.6073\n",
      "Epoch [113/1000], Training Loss: 3.3963, Test Loss: 79.6864\n",
      "Epoch [114/1000], Training Loss: 3.3082, Test Loss: 69.8731\n",
      "Epoch [115/1000], Training Loss: 2.8009, Test Loss: 73.7882\n",
      "Epoch [116/1000], Training Loss: 3.0492, Test Loss: 74.2353\n",
      "Epoch [117/1000], Training Loss: 3.0027, Test Loss: 70.8787\n",
      "Epoch [118/1000], Training Loss: 2.7883, Test Loss: 67.1724\n",
      "Epoch [119/1000], Training Loss: 3.6145, Test Loss: 82.0099\n",
      "Epoch [120/1000], Training Loss: 4.0850, Test Loss: 69.4071\n",
      "Epoch [121/1000], Training Loss: 3.4261, Test Loss: 75.5183\n",
      "Epoch [122/1000], Training Loss: 4.5573, Test Loss: 69.0047\n",
      "Epoch [123/1000], Training Loss: 3.3732, Test Loss: 71.0533\n",
      "Epoch [124/1000], Training Loss: 3.2240, Test Loss: 72.6553\n",
      "Epoch [125/1000], Training Loss: 4.7480, Test Loss: 80.6023\n",
      "Epoch [126/1000], Training Loss: 3.1641, Test Loss: 78.6579\n",
      "Epoch [127/1000], Training Loss: 5.4224, Test Loss: 71.9923\n",
      "Epoch [128/1000], Training Loss: 2.8752, Test Loss: 73.0236\n",
      "Epoch [129/1000], Training Loss: 3.2791, Test Loss: 70.9500\n",
      "Epoch [130/1000], Training Loss: 3.0862, Test Loss: 78.0664\n",
      "Epoch [131/1000], Training Loss: 2.8090, Test Loss: 83.7134\n",
      "Epoch [132/1000], Training Loss: 6.3890, Test Loss: 67.1655\n",
      "Epoch [133/1000], Training Loss: 2.0994, Test Loss: 71.2829\n",
      "Epoch [134/1000], Training Loss: 2.4560, Test Loss: 84.7549\n",
      "Epoch [135/1000], Training Loss: 4.2221, Test Loss: 85.4272\n",
      "Epoch [136/1000], Training Loss: 2.9014, Test Loss: 78.9144\n",
      "Epoch [137/1000], Training Loss: 2.7206, Test Loss: 74.0550\n",
      "Epoch [138/1000], Training Loss: 2.9102, Test Loss: 66.8473\n",
      "Epoch [139/1000], Training Loss: 2.4930, Test Loss: 67.3134\n",
      "Epoch [140/1000], Training Loss: 1.5838, Test Loss: 74.2263\n",
      "Epoch [141/1000], Training Loss: 2.3025, Test Loss: 64.5453\n",
      "Epoch [142/1000], Training Loss: 3.0683, Test Loss: 63.6835\n",
      "Epoch [143/1000], Training Loss: 3.0683, Test Loss: 70.4243\n",
      "Epoch [144/1000], Training Loss: 2.6758, Test Loss: 79.5741\n",
      "Epoch [145/1000], Training Loss: 4.2026, Test Loss: 72.5942\n",
      "Epoch [146/1000], Training Loss: 2.9911, Test Loss: 64.6127\n",
      "Epoch [147/1000], Training Loss: 3.0304, Test Loss: 74.3285\n",
      "Epoch [148/1000], Training Loss: 3.1628, Test Loss: 76.1657\n",
      "Epoch [149/1000], Training Loss: 3.4521, Test Loss: 68.7677\n",
      "Epoch [150/1000], Training Loss: 2.4343, Test Loss: 60.5803\n",
      "Epoch [151/1000], Training Loss: 2.2525, Test Loss: 83.4617\n",
      "Epoch [152/1000], Training Loss: 2.9658, Test Loss: 79.8149\n",
      "Epoch [153/1000], Training Loss: 2.1022, Test Loss: 72.0858\n",
      "Epoch [154/1000], Training Loss: 1.7163, Test Loss: 77.0899\n",
      "Epoch [155/1000], Training Loss: 2.3135, Test Loss: 75.8071\n",
      "Epoch [156/1000], Training Loss: 2.7324, Test Loss: 69.0854\n",
      "Epoch [157/1000], Training Loss: 3.1916, Test Loss: 72.2760\n",
      "Epoch [158/1000], Training Loss: 2.1519, Test Loss: 75.1381\n",
      "Epoch [159/1000], Training Loss: 2.1582, Test Loss: 76.3277\n",
      "Epoch [160/1000], Training Loss: 2.6723, Test Loss: 84.4652\n",
      "Epoch [161/1000], Training Loss: 2.5393, Test Loss: 80.9739\n",
      "Epoch [162/1000], Training Loss: 2.0831, Test Loss: 91.9319\n",
      "Epoch [163/1000], Training Loss: 4.2961, Test Loss: 82.1907\n",
      "Epoch [164/1000], Training Loss: 2.3002, Test Loss: 75.2518\n",
      "Epoch [165/1000], Training Loss: 2.3474, Test Loss: 72.1988\n",
      "Epoch [166/1000], Training Loss: 2.2735, Test Loss: 74.3986\n",
      "Epoch [167/1000], Training Loss: 2.3643, Test Loss: 72.4793\n",
      "Epoch [168/1000], Training Loss: 3.8519, Test Loss: 64.2090\n",
      "Epoch [169/1000], Training Loss: 4.7602, Test Loss: 57.5336\n",
      "Epoch [170/1000], Training Loss: 2.7932, Test Loss: 79.5844\n",
      "Epoch [171/1000], Training Loss: 2.6698, Test Loss: 65.8424\n",
      "Epoch [172/1000], Training Loss: 1.6604, Test Loss: 71.6073\n",
      "Epoch [173/1000], Training Loss: 2.2828, Test Loss: 71.7117\n",
      "Epoch [174/1000], Training Loss: 4.6466, Test Loss: 81.0235\n",
      "Epoch [175/1000], Training Loss: 2.4749, Test Loss: 70.4329\n",
      "Epoch [176/1000], Training Loss: 1.4523, Test Loss: 68.8486\n",
      "Epoch [177/1000], Training Loss: 1.6225, Test Loss: 69.3602\n",
      "Epoch [178/1000], Training Loss: 1.4518, Test Loss: 74.2492\n",
      "Epoch [179/1000], Training Loss: 1.7255, Test Loss: 69.2066\n",
      "Epoch [180/1000], Training Loss: 2.5331, Test Loss: 69.3707\n",
      "Epoch [181/1000], Training Loss: 2.1543, Test Loss: 71.9183\n",
      "Epoch [182/1000], Training Loss: 2.1455, Test Loss: 73.1284\n",
      "Epoch [183/1000], Training Loss: 1.6117, Test Loss: 80.5041\n",
      "Epoch [184/1000], Training Loss: 2.1850, Test Loss: 72.0124\n",
      "Epoch [185/1000], Training Loss: 1.9399, Test Loss: 67.3602\n",
      "Epoch [186/1000], Training Loss: 2.2874, Test Loss: 82.8001\n",
      "Epoch [187/1000], Training Loss: 3.0320, Test Loss: 70.0555\n",
      "Epoch [188/1000], Training Loss: 2.6050, Test Loss: 65.0094\n",
      "Epoch [189/1000], Training Loss: 2.9445, Test Loss: 77.2336\n",
      "Epoch [190/1000], Training Loss: 1.8793, Test Loss: 65.4488\n",
      "Epoch [191/1000], Training Loss: 2.0119, Test Loss: 80.8909\n",
      "Epoch [192/1000], Training Loss: 2.0893, Test Loss: 70.5181\n",
      "Epoch [193/1000], Training Loss: 2.7487, Test Loss: 68.5090\n",
      "Epoch [194/1000], Training Loss: 2.1187, Test Loss: 87.7596\n",
      "Epoch [195/1000], Training Loss: 1.5531, Test Loss: 76.0595\n",
      "Epoch [196/1000], Training Loss: 2.4900, Test Loss: 63.6399\n",
      "Epoch [197/1000], Training Loss: 1.7447, Test Loss: 72.4868\n",
      "Epoch [198/1000], Training Loss: 1.8392, Test Loss: 59.8817\n",
      "Epoch [199/1000], Training Loss: 3.1321, Test Loss: 78.2878\n",
      "Epoch [200/1000], Training Loss: 1.5011, Test Loss: 72.9221\n",
      "Epoch [201/1000], Training Loss: 1.4895, Test Loss: 71.2045\n",
      "Epoch [202/1000], Training Loss: 2.2419, Test Loss: 72.3220\n",
      "Epoch [203/1000], Training Loss: 1.6536, Test Loss: 74.1783\n",
      "Epoch [204/1000], Training Loss: 1.5032, Test Loss: 72.7314\n",
      "Epoch [205/1000], Training Loss: 1.8219, Test Loss: 77.5380\n",
      "Epoch [206/1000], Training Loss: 2.4417, Test Loss: 73.2010\n",
      "Epoch [207/1000], Training Loss: 2.0256, Test Loss: 72.0621\n",
      "Epoch [208/1000], Training Loss: 1.2519, Test Loss: 70.0418\n",
      "Epoch [209/1000], Training Loss: 2.3962, Test Loss: 76.0843\n",
      "Epoch [210/1000], Training Loss: 2.8287, Test Loss: 74.4249\n",
      "Epoch [211/1000], Training Loss: 2.3565, Test Loss: 75.8361\n",
      "Epoch [212/1000], Training Loss: 1.5109, Test Loss: 74.9171\n",
      "Epoch [213/1000], Training Loss: 1.2521, Test Loss: 74.9234\n",
      "Epoch [214/1000], Training Loss: 1.9870, Test Loss: 72.4744\n",
      "Epoch [215/1000], Training Loss: 1.6555, Test Loss: 74.6159\n",
      "Epoch [216/1000], Training Loss: 1.1363, Test Loss: 75.5803\n",
      "Epoch [217/1000], Training Loss: 1.1772, Test Loss: 71.4178\n",
      "Epoch [218/1000], Training Loss: 1.1327, Test Loss: 74.3649\n",
      "Epoch [219/1000], Training Loss: 1.2448, Test Loss: 71.5560\n",
      "Epoch [220/1000], Training Loss: 1.9190, Test Loss: 75.3892\n",
      "Epoch [221/1000], Training Loss: 3.7449, Test Loss: 70.0342\n",
      "Epoch [222/1000], Training Loss: 1.4120, Test Loss: 73.9571\n",
      "Epoch [223/1000], Training Loss: 1.2896, Test Loss: 69.6499\n",
      "Epoch [224/1000], Training Loss: 0.8822, Test Loss: 76.8869\n",
      "Epoch [225/1000], Training Loss: 1.9442, Test Loss: 75.4903\n",
      "Epoch [226/1000], Training Loss: 1.4651, Test Loss: 75.5940\n",
      "Epoch [227/1000], Training Loss: 1.3212, Test Loss: 76.0510\n",
      "Epoch [228/1000], Training Loss: 1.1925, Test Loss: 77.9799\n",
      "Epoch [229/1000], Training Loss: 1.5354, Test Loss: 77.4608\n",
      "Epoch [230/1000], Training Loss: 1.6054, Test Loss: 76.4907\n",
      "Epoch [231/1000], Training Loss: 0.9637, Test Loss: 75.9515\n",
      "Epoch [232/1000], Training Loss: 1.2285, Test Loss: 72.3176\n",
      "Epoch [233/1000], Training Loss: 0.9395, Test Loss: 72.6290\n",
      "Epoch [234/1000], Training Loss: 1.6714, Test Loss: 75.1199\n",
      "Epoch [235/1000], Training Loss: 1.4397, Test Loss: 74.5757\n",
      "Epoch [236/1000], Training Loss: 1.0164, Test Loss: 74.2846\n",
      "Epoch [237/1000], Training Loss: 1.0221, Test Loss: 76.0628\n",
      "Epoch [238/1000], Training Loss: 0.9988, Test Loss: 75.8359\n",
      "Epoch [239/1000], Training Loss: 0.9128, Test Loss: 77.0646\n",
      "Epoch [240/1000], Training Loss: 1.2164, Test Loss: 75.7581\n",
      "Epoch [241/1000], Training Loss: 1.1430, Test Loss: 74.0403\n",
      "Epoch [242/1000], Training Loss: 1.4415, Test Loss: 76.6246\n",
      "Epoch [243/1000], Training Loss: 1.1876, Test Loss: 77.1305\n",
      "Epoch [244/1000], Training Loss: 1.4741, Test Loss: 78.1765\n",
      "Epoch [245/1000], Training Loss: 2.6143, Test Loss: 75.1653\n",
      "Epoch [246/1000], Training Loss: 2.6596, Test Loss: 77.4152\n",
      "Epoch [247/1000], Training Loss: 1.4681, Test Loss: 74.5816\n",
      "Epoch [248/1000], Training Loss: 1.1105, Test Loss: 78.6489\n",
      "Epoch [249/1000], Training Loss: 1.6501, Test Loss: 74.8001\n",
      "Epoch [250/1000], Training Loss: 1.3645, Test Loss: 74.9851\n",
      "Epoch [251/1000], Training Loss: 1.3663, Test Loss: 75.1543\n",
      "Epoch [252/1000], Training Loss: 1.3198, Test Loss: 72.2864\n",
      "Epoch [253/1000], Training Loss: 1.1664, Test Loss: 77.5915\n",
      "Epoch [254/1000], Training Loss: 1.6786, Test Loss: 76.3143\n",
      "Epoch [255/1000], Training Loss: 1.4140, Test Loss: 76.5629\n",
      "Epoch [256/1000], Training Loss: 1.2171, Test Loss: 77.4266\n",
      "Epoch [257/1000], Training Loss: 3.1044, Test Loss: 80.1986\n",
      "Epoch [258/1000], Training Loss: 1.7284, Test Loss: 75.8249\n",
      "Epoch [259/1000], Training Loss: 1.9621, Test Loss: 79.1042\n",
      "Epoch [260/1000], Training Loss: 1.1269, Test Loss: 81.3485\n",
      "Epoch [261/1000], Training Loss: 1.3346, Test Loss: 75.0649\n",
      "Epoch [262/1000], Training Loss: 1.6234, Test Loss: 76.3695\n",
      "Epoch [263/1000], Training Loss: 0.6422, Test Loss: 75.5023\n",
      "Epoch [264/1000], Training Loss: 1.6178, Test Loss: 76.4770\n",
      "Epoch [265/1000], Training Loss: 3.9365, Test Loss: 76.3325\n",
      "Epoch [266/1000], Training Loss: 1.0097, Test Loss: 77.7718\n",
      "Epoch [267/1000], Training Loss: 0.9019, Test Loss: 78.4108\n",
      "Epoch [268/1000], Training Loss: 1.1296, Test Loss: 77.6447\n",
      "Epoch [269/1000], Training Loss: 0.6765, Test Loss: 77.6756\n",
      "Epoch [270/1000], Training Loss: 1.8493, Test Loss: 75.2143\n",
      "Epoch [271/1000], Training Loss: 1.1734, Test Loss: 78.1354\n",
      "Epoch [272/1000], Training Loss: 1.1708, Test Loss: 73.0449\n",
      "Epoch [273/1000], Training Loss: 1.7211, Test Loss: 74.0009\n",
      "Epoch [274/1000], Training Loss: 2.4895, Test Loss: 72.1518\n",
      "Epoch [275/1000], Training Loss: 1.0076, Test Loss: 75.1725\n",
      "Epoch [276/1000], Training Loss: 0.8250, Test Loss: 74.5723\n",
      "Epoch [277/1000], Training Loss: 2.0741, Test Loss: 73.8593\n",
      "Epoch [278/1000], Training Loss: 2.3565, Test Loss: 73.5658\n",
      "Epoch [279/1000], Training Loss: 0.9021, Test Loss: 77.9185\n",
      "Epoch [280/1000], Training Loss: 1.6251, Test Loss: 74.3258\n",
      "Epoch [281/1000], Training Loss: 0.7552, Test Loss: 76.7693\n",
      "Epoch [282/1000], Training Loss: 1.1429, Test Loss: 76.1356\n",
      "Epoch [283/1000], Training Loss: 0.8093, Test Loss: 78.6069\n",
      "Epoch [284/1000], Training Loss: 0.6897, Test Loss: 76.2670\n",
      "Epoch [285/1000], Training Loss: 0.9501, Test Loss: 78.4377\n",
      "Epoch [286/1000], Training Loss: 1.8222, Test Loss: 72.8253\n",
      "Epoch [287/1000], Training Loss: 1.0348, Test Loss: 74.7964\n",
      "Epoch [288/1000], Training Loss: 0.7499, Test Loss: 82.5527\n",
      "Epoch [289/1000], Training Loss: 2.1238, Test Loss: 79.8355\n",
      "Epoch [290/1000], Training Loss: 0.8584, Test Loss: 73.8108\n",
      "Epoch [291/1000], Training Loss: 1.4510, Test Loss: 77.1998\n",
      "Epoch [292/1000], Training Loss: 1.1544, Test Loss: 77.8275\n",
      "Epoch [293/1000], Training Loss: 0.8047, Test Loss: 77.5418\n",
      "Epoch [294/1000], Training Loss: 1.3343, Test Loss: 74.2647\n",
      "Epoch [295/1000], Training Loss: 1.4190, Test Loss: 77.2747\n",
      "Epoch [296/1000], Training Loss: 2.0240, Test Loss: 76.5901\n",
      "Epoch [297/1000], Training Loss: 0.9641, Test Loss: 76.1651\n",
      "Epoch [298/1000], Training Loss: 1.7685, Test Loss: 73.5024\n",
      "Epoch [299/1000], Training Loss: 1.0817, Test Loss: 77.5046\n",
      "Epoch [300/1000], Training Loss: 3.0937, Test Loss: 72.4271\n",
      "Epoch [301/1000], Training Loss: 1.0746, Test Loss: 74.2272\n",
      "Epoch [302/1000], Training Loss: 0.9309, Test Loss: 75.3810\n",
      "Epoch [303/1000], Training Loss: 0.6269, Test Loss: 75.6829\n",
      "Epoch [304/1000], Training Loss: 1.0137, Test Loss: 73.4684\n",
      "Epoch [305/1000], Training Loss: 0.8914, Test Loss: 74.5660\n",
      "Epoch [306/1000], Training Loss: 0.8677, Test Loss: 75.0258\n",
      "Epoch [307/1000], Training Loss: 1.0264, Test Loss: 75.9483\n",
      "Epoch [308/1000], Training Loss: 0.6981, Test Loss: 77.8118\n",
      "Epoch [309/1000], Training Loss: 1.1305, Test Loss: 74.7511\n",
      "Epoch [310/1000], Training Loss: 1.1745, Test Loss: 76.0821\n",
      "Epoch [311/1000], Training Loss: 0.7873, Test Loss: 77.4026\n",
      "Epoch [312/1000], Training Loss: 1.0123, Test Loss: 75.6398\n",
      "Epoch [313/1000], Training Loss: 0.7890, Test Loss: 75.5579\n",
      "Epoch [314/1000], Training Loss: 1.8334, Test Loss: 76.2715\n",
      "Epoch [315/1000], Training Loss: 0.9782, Test Loss: 73.4357\n",
      "Epoch [316/1000], Training Loss: 1.3266, Test Loss: 74.2628\n",
      "Epoch [317/1000], Training Loss: 1.1809, Test Loss: 75.4451\n",
      "Epoch [318/1000], Training Loss: 1.1972, Test Loss: 75.6915\n",
      "Epoch [319/1000], Training Loss: 1.2146, Test Loss: 77.3933\n",
      "Epoch [320/1000], Training Loss: 1.8589, Test Loss: 75.3848\n",
      "Epoch [321/1000], Training Loss: 1.0319, Test Loss: 77.8968\n",
      "Epoch [322/1000], Training Loss: 0.7505, Test Loss: 74.9017\n",
      "Epoch [323/1000], Training Loss: 0.9302, Test Loss: 74.9074\n",
      "Epoch [324/1000], Training Loss: 0.7078, Test Loss: 77.2196\n",
      "Epoch [325/1000], Training Loss: 2.2067, Test Loss: 75.9931\n",
      "Epoch [326/1000], Training Loss: 1.5793, Test Loss: 77.4195\n",
      "Epoch [327/1000], Training Loss: 0.8030, Test Loss: 77.6653\n",
      "Epoch [328/1000], Training Loss: 1.1206, Test Loss: 74.1938\n",
      "Epoch [329/1000], Training Loss: 1.9307, Test Loss: 75.5685\n",
      "Epoch [330/1000], Training Loss: 0.9056, Test Loss: 75.6619\n",
      "Epoch [331/1000], Training Loss: 2.2335, Test Loss: 74.9835\n",
      "Epoch [332/1000], Training Loss: 1.0228, Test Loss: 74.9594\n",
      "Epoch [333/1000], Training Loss: 1.2889, Test Loss: 76.4744\n",
      "Epoch [334/1000], Training Loss: 1.9398, Test Loss: 74.5113\n",
      "Epoch [335/1000], Training Loss: 0.7899, Test Loss: 75.6433\n",
      "Epoch [336/1000], Training Loss: 2.0525, Test Loss: 76.2122\n",
      "Epoch [337/1000], Training Loss: 1.6595, Test Loss: 76.1163\n",
      "Epoch [338/1000], Training Loss: 0.8723, Test Loss: 76.0911\n",
      "Epoch [339/1000], Training Loss: 0.9221, Test Loss: 76.7238\n",
      "Epoch [340/1000], Training Loss: 0.6107, Test Loss: 76.8673\n",
      "Epoch [341/1000], Training Loss: 1.3540, Test Loss: 76.3552\n",
      "Epoch [342/1000], Training Loss: 1.4211, Test Loss: 75.1319\n",
      "Epoch [343/1000], Training Loss: 2.9884, Test Loss: 76.0266\n",
      "Epoch [344/1000], Training Loss: 2.5118, Test Loss: 75.3901\n",
      "Epoch [345/1000], Training Loss: 1.4566, Test Loss: 75.8009\n",
      "Epoch [346/1000], Training Loss: 1.1729, Test Loss: 75.6564\n",
      "Epoch [347/1000], Training Loss: 0.7776, Test Loss: 77.7346\n",
      "Epoch [348/1000], Training Loss: 1.1271, Test Loss: 76.7944\n",
      "Epoch [349/1000], Training Loss: 1.2106, Test Loss: 77.4123\n",
      "Epoch [350/1000], Training Loss: 0.9245, Test Loss: 77.1741\n",
      "Epoch [351/1000], Training Loss: 0.7306, Test Loss: 77.2801\n",
      "Epoch [352/1000], Training Loss: 0.7754, Test Loss: 74.7008\n",
      "Epoch [353/1000], Training Loss: 0.7691, Test Loss: 74.7620\n",
      "Epoch [354/1000], Training Loss: 2.2738, Test Loss: 78.3973\n",
      "Epoch [355/1000], Training Loss: 1.5723, Test Loss: 77.4513\n",
      "Epoch [356/1000], Training Loss: 1.2802, Test Loss: 76.2486\n",
      "Epoch [357/1000], Training Loss: 1.1778, Test Loss: 77.2378\n",
      "Epoch [358/1000], Training Loss: 1.2163, Test Loss: 73.8319\n",
      "Epoch [359/1000], Training Loss: 1.3930, Test Loss: 76.2414\n",
      "Epoch [360/1000], Training Loss: 1.0230, Test Loss: 77.2988\n",
      "Epoch [361/1000], Training Loss: 1.0590, Test Loss: 75.8428\n",
      "Epoch [362/1000], Training Loss: 0.7350, Test Loss: 76.8525\n",
      "Epoch [363/1000], Training Loss: 0.7106, Test Loss: 74.0268\n",
      "Epoch [364/1000], Training Loss: 2.1390, Test Loss: 74.5022\n",
      "Epoch [365/1000], Training Loss: 1.5871, Test Loss: 75.5010\n",
      "Epoch [366/1000], Training Loss: 0.9597, Test Loss: 75.4144\n",
      "Epoch [367/1000], Training Loss: 0.6029, Test Loss: 75.6324\n",
      "Epoch [368/1000], Training Loss: 1.0706, Test Loss: 76.6447\n",
      "Epoch [369/1000], Training Loss: 0.6870, Test Loss: 75.7569\n",
      "Epoch [370/1000], Training Loss: 0.9874, Test Loss: 74.5209\n",
      "Epoch [371/1000], Training Loss: 1.5185, Test Loss: 76.4897\n",
      "Epoch [372/1000], Training Loss: 0.8360, Test Loss: 75.8781\n",
      "Epoch [373/1000], Training Loss: 1.1827, Test Loss: 75.6071\n",
      "Epoch [374/1000], Training Loss: 1.1953, Test Loss: 75.7317\n",
      "Epoch [375/1000], Training Loss: 0.6971, Test Loss: 75.2512\n",
      "Epoch [376/1000], Training Loss: 1.9401, Test Loss: 75.0854\n",
      "Epoch [377/1000], Training Loss: 0.7019, Test Loss: 75.5999\n",
      "Epoch [378/1000], Training Loss: 1.3284, Test Loss: 75.5628\n",
      "Epoch [379/1000], Training Loss: 1.1542, Test Loss: 76.7565\n",
      "Epoch [380/1000], Training Loss: 0.8405, Test Loss: 77.5338\n",
      "Epoch [381/1000], Training Loss: 0.5889, Test Loss: 75.7365\n",
      "Epoch [382/1000], Training Loss: 0.7833, Test Loss: 75.4547\n",
      "Epoch [383/1000], Training Loss: 1.2392, Test Loss: 74.2944\n",
      "Epoch [384/1000], Training Loss: 0.6947, Test Loss: 75.2996\n",
      "Epoch [385/1000], Training Loss: 1.0347, Test Loss: 77.4769\n",
      "Epoch [386/1000], Training Loss: 1.0750, Test Loss: 75.6294\n",
      "Epoch [387/1000], Training Loss: 0.9432, Test Loss: 76.4304\n",
      "Epoch [388/1000], Training Loss: 2.7422, Test Loss: 75.0175\n",
      "Epoch [389/1000], Training Loss: 1.0590, Test Loss: 77.0992\n",
      "Epoch [390/1000], Training Loss: 0.6182, Test Loss: 75.3370\n",
      "Epoch [391/1000], Training Loss: 1.3966, Test Loss: 75.7892\n",
      "Epoch [392/1000], Training Loss: 0.9963, Test Loss: 75.2053\n",
      "Epoch [393/1000], Training Loss: 1.4808, Test Loss: 77.1355\n",
      "Epoch [394/1000], Training Loss: 0.5713, Test Loss: 76.3278\n",
      "Epoch [395/1000], Training Loss: 0.7251, Test Loss: 74.5165\n",
      "Epoch [396/1000], Training Loss: 1.3119, Test Loss: 77.1881\n",
      "Epoch [397/1000], Training Loss: 1.5572, Test Loss: 77.4608\n",
      "Epoch [398/1000], Training Loss: 1.4586, Test Loss: 78.0382\n",
      "Epoch [399/1000], Training Loss: 1.3753, Test Loss: 74.8837\n",
      "Epoch [400/1000], Training Loss: 1.7202, Test Loss: 74.1742\n",
      "Epoch [401/1000], Training Loss: 1.9810, Test Loss: 76.7435\n",
      "Epoch [402/1000], Training Loss: 0.9355, Test Loss: 75.0346\n",
      "Epoch [403/1000], Training Loss: 1.0523, Test Loss: 76.6611\n",
      "Epoch [404/1000], Training Loss: 1.1645, Test Loss: 75.4969\n",
      "Epoch [405/1000], Training Loss: 1.6746, Test Loss: 75.1122\n",
      "Epoch [406/1000], Training Loss: 0.9951, Test Loss: 75.6053\n",
      "Epoch [407/1000], Training Loss: 0.6785, Test Loss: 75.3506\n",
      "Epoch [408/1000], Training Loss: 0.8276, Test Loss: 75.9031\n",
      "Epoch [409/1000], Training Loss: 2.4272, Test Loss: 77.0226\n",
      "Epoch [410/1000], Training Loss: 0.7435, Test Loss: 75.6693\n",
      "Epoch [411/1000], Training Loss: 1.3164, Test Loss: 77.6122\n",
      "Epoch [412/1000], Training Loss: 0.7515, Test Loss: 76.9054\n",
      "Epoch [413/1000], Training Loss: 0.7187, Test Loss: 74.2725\n",
      "Epoch [414/1000], Training Loss: 0.7631, Test Loss: 76.6502\n",
      "Epoch [415/1000], Training Loss: 0.8112, Test Loss: 76.1377\n",
      "Epoch [416/1000], Training Loss: 0.7043, Test Loss: 76.6733\n",
      "Epoch [417/1000], Training Loss: 1.2076, Test Loss: 76.9374\n",
      "Epoch [418/1000], Training Loss: 1.8150, Test Loss: 74.5812\n",
      "Epoch [419/1000], Training Loss: 0.8058, Test Loss: 76.9723\n",
      "Epoch [420/1000], Training Loss: 1.2126, Test Loss: 74.3729\n",
      "Epoch [421/1000], Training Loss: 0.8195, Test Loss: 76.8620\n",
      "Epoch [422/1000], Training Loss: 2.1034, Test Loss: 74.7578\n",
      "Epoch [423/1000], Training Loss: 1.3082, Test Loss: 74.6085\n",
      "Epoch [424/1000], Training Loss: 0.8582, Test Loss: 77.0550\n",
      "Epoch [425/1000], Training Loss: 0.9077, Test Loss: 76.0731\n",
      "Epoch [426/1000], Training Loss: 1.2442, Test Loss: 76.7322\n",
      "Epoch [427/1000], Training Loss: 1.5895, Test Loss: 76.0256\n",
      "Epoch [428/1000], Training Loss: 0.7370, Test Loss: 74.2988\n",
      "Epoch [429/1000], Training Loss: 1.6096, Test Loss: 75.4574\n",
      "Epoch [430/1000], Training Loss: 0.5985, Test Loss: 74.8970\n",
      "Epoch [431/1000], Training Loss: 0.7585, Test Loss: 75.2986\n",
      "Epoch [432/1000], Training Loss: 1.5515, Test Loss: 76.1592\n",
      "Epoch [433/1000], Training Loss: 0.9221, Test Loss: 76.1322\n",
      "Epoch [434/1000], Training Loss: 1.1864, Test Loss: 76.0100\n",
      "Epoch [435/1000], Training Loss: 0.6885, Test Loss: 74.3396\n",
      "Epoch [436/1000], Training Loss: 0.8704, Test Loss: 74.6901\n",
      "Epoch [437/1000], Training Loss: 1.6175, Test Loss: 75.6662\n",
      "Epoch [438/1000], Training Loss: 0.7192, Test Loss: 75.0502\n",
      "Epoch [439/1000], Training Loss: 1.3393, Test Loss: 75.4511\n",
      "Epoch [440/1000], Training Loss: 1.5115, Test Loss: 76.4577\n",
      "Epoch [441/1000], Training Loss: 0.6430, Test Loss: 75.7074\n",
      "Epoch [442/1000], Training Loss: 1.9037, Test Loss: 76.8145\n",
      "Epoch [443/1000], Training Loss: 1.3174, Test Loss: 74.9681\n",
      "Epoch [444/1000], Training Loss: 0.7444, Test Loss: 74.5814\n",
      "Epoch [445/1000], Training Loss: 0.9488, Test Loss: 76.6269\n",
      "Epoch [446/1000], Training Loss: 1.9858, Test Loss: 75.3156\n",
      "Epoch [447/1000], Training Loss: 1.4526, Test Loss: 74.4301\n",
      "Epoch [448/1000], Training Loss: 1.5384, Test Loss: 75.4668\n",
      "Epoch [449/1000], Training Loss: 1.4597, Test Loss: 77.9942\n",
      "Epoch [450/1000], Training Loss: 0.8889, Test Loss: 76.4638\n",
      "Epoch [451/1000], Training Loss: 0.6082, Test Loss: 76.7548\n",
      "Epoch [452/1000], Training Loss: 0.7162, Test Loss: 76.7566\n",
      "Epoch [453/1000], Training Loss: 0.9732, Test Loss: 76.7309\n",
      "Epoch [454/1000], Training Loss: 0.4861, Test Loss: 75.5229\n",
      "Epoch [455/1000], Training Loss: 1.1571, Test Loss: 75.2278\n",
      "Epoch [456/1000], Training Loss: 0.7599, Test Loss: 75.8888\n",
      "Epoch [457/1000], Training Loss: 0.6338, Test Loss: 75.4816\n",
      "Epoch [458/1000], Training Loss: 0.8672, Test Loss: 74.7967\n",
      "Epoch [459/1000], Training Loss: 0.9758, Test Loss: 75.8844\n",
      "Epoch [460/1000], Training Loss: 0.7729, Test Loss: 75.7972\n",
      "Epoch [461/1000], Training Loss: 0.9385, Test Loss: 77.1430\n",
      "Epoch [462/1000], Training Loss: 0.8355, Test Loss: 75.4619\n",
      "Epoch [463/1000], Training Loss: 1.3709, Test Loss: 76.5311\n",
      "Epoch [464/1000], Training Loss: 0.9413, Test Loss: 77.1109\n",
      "Epoch [465/1000], Training Loss: 0.9667, Test Loss: 75.9274\n",
      "Epoch [466/1000], Training Loss: 0.7801, Test Loss: 75.1975\n",
      "Epoch [467/1000], Training Loss: 2.0825, Test Loss: 75.8095\n",
      "Epoch [468/1000], Training Loss: 0.7187, Test Loss: 75.3431\n",
      "Epoch [469/1000], Training Loss: 1.0582, Test Loss: 76.4000\n",
      "Epoch [470/1000], Training Loss: 0.6968, Test Loss: 75.2464\n",
      "Epoch [471/1000], Training Loss: 0.7870, Test Loss: 75.7217\n",
      "Epoch [472/1000], Training Loss: 0.8911, Test Loss: 76.3292\n",
      "Epoch [473/1000], Training Loss: 0.5734, Test Loss: 76.5125\n",
      "Epoch [474/1000], Training Loss: 0.9029, Test Loss: 75.8454\n",
      "Epoch [475/1000], Training Loss: 0.7304, Test Loss: 75.7757\n",
      "Epoch [476/1000], Training Loss: 1.6482, Test Loss: 76.8803\n",
      "Epoch [477/1000], Training Loss: 1.0743, Test Loss: 76.9106\n",
      "Epoch [478/1000], Training Loss: 0.8143, Test Loss: 75.4702\n",
      "Epoch [479/1000], Training Loss: 2.2790, Test Loss: 76.1254\n",
      "Epoch [480/1000], Training Loss: 0.6954, Test Loss: 74.0406\n",
      "Epoch [481/1000], Training Loss: 1.0846, Test Loss: 75.5361\n",
      "Epoch [482/1000], Training Loss: 0.6580, Test Loss: 76.4615\n",
      "Epoch [483/1000], Training Loss: 0.7329, Test Loss: 76.1735\n",
      "Epoch [484/1000], Training Loss: 1.1547, Test Loss: 76.0392\n",
      "Epoch [485/1000], Training Loss: 1.2442, Test Loss: 75.4406\n",
      "Epoch [486/1000], Training Loss: 1.2127, Test Loss: 76.6093\n",
      "Epoch [487/1000], Training Loss: 1.7399, Test Loss: 74.9155\n",
      "Epoch [488/1000], Training Loss: 1.0838, Test Loss: 76.2900\n",
      "Epoch [489/1000], Training Loss: 1.1620, Test Loss: 74.5163\n",
      "Epoch [490/1000], Training Loss: 1.6117, Test Loss: 74.7966\n",
      "Epoch [491/1000], Training Loss: 0.6732, Test Loss: 75.6906\n",
      "Epoch [492/1000], Training Loss: 0.9477, Test Loss: 75.6193\n",
      "Epoch [493/1000], Training Loss: 1.3732, Test Loss: 73.9494\n",
      "Epoch [494/1000], Training Loss: 2.4221, Test Loss: 77.1042\n",
      "Epoch [495/1000], Training Loss: 1.6319, Test Loss: 76.3693\n",
      "Epoch [496/1000], Training Loss: 1.3469, Test Loss: 78.0383\n",
      "Epoch [497/1000], Training Loss: 1.0816, Test Loss: 76.5087\n",
      "Epoch [498/1000], Training Loss: 1.1195, Test Loss: 74.8559\n",
      "Epoch [499/1000], Training Loss: 1.1483, Test Loss: 76.3939\n",
      "Epoch [500/1000], Training Loss: 0.9286, Test Loss: 75.7077\n",
      "Epoch [501/1000], Training Loss: 1.1968, Test Loss: 75.3370\n",
      "Epoch [502/1000], Training Loss: 2.2495, Test Loss: 76.4672\n",
      "Epoch [503/1000], Training Loss: 0.8782, Test Loss: 76.7671\n",
      "Epoch [504/1000], Training Loss: 0.7165, Test Loss: 76.3095\n",
      "Epoch [505/1000], Training Loss: 0.5031, Test Loss: 73.3313\n",
      "Epoch [506/1000], Training Loss: 0.8939, Test Loss: 74.6672\n",
      "Epoch [507/1000], Training Loss: 0.7244, Test Loss: 76.4107\n",
      "Epoch [508/1000], Training Loss: 0.8274, Test Loss: 77.9876\n",
      "Epoch [509/1000], Training Loss: 1.3565, Test Loss: 75.9239\n",
      "Epoch [510/1000], Training Loss: 2.1067, Test Loss: 74.4281\n",
      "Epoch [511/1000], Training Loss: 0.7818, Test Loss: 75.9002\n",
      "Epoch [512/1000], Training Loss: 1.0638, Test Loss: 76.9026\n",
      "Epoch [513/1000], Training Loss: 1.1316, Test Loss: 75.7675\n",
      "Epoch [514/1000], Training Loss: 1.2789, Test Loss: 74.2726\n",
      "Epoch [515/1000], Training Loss: 1.1021, Test Loss: 75.5751\n",
      "Epoch [516/1000], Training Loss: 0.5726, Test Loss: 76.2285\n",
      "Epoch [517/1000], Training Loss: 1.0152, Test Loss: 76.0581\n",
      "Epoch [518/1000], Training Loss: 0.8050, Test Loss: 74.3558\n",
      "Epoch [519/1000], Training Loss: 1.0405, Test Loss: 75.3517\n",
      "Epoch [520/1000], Training Loss: 0.7261, Test Loss: 75.1963\n",
      "Epoch [521/1000], Training Loss: 1.0260, Test Loss: 75.4553\n",
      "Epoch [522/1000], Training Loss: 2.0820, Test Loss: 74.7137\n",
      "Epoch [523/1000], Training Loss: 1.4154, Test Loss: 74.8617\n",
      "Epoch [524/1000], Training Loss: 1.1386, Test Loss: 77.1138\n",
      "Epoch [525/1000], Training Loss: 0.8172, Test Loss: 74.3935\n",
      "Epoch [526/1000], Training Loss: 2.2150, Test Loss: 74.9904\n",
      "Epoch [527/1000], Training Loss: 0.7955, Test Loss: 76.0314\n",
      "Epoch [528/1000], Training Loss: 1.0579, Test Loss: 76.0383\n",
      "Epoch [529/1000], Training Loss: 0.8803, Test Loss: 76.1065\n",
      "Epoch [530/1000], Training Loss: 0.8295, Test Loss: 77.2705\n",
      "Epoch [531/1000], Training Loss: 1.2970, Test Loss: 76.7553\n",
      "Epoch [532/1000], Training Loss: 0.8220, Test Loss: 75.0539\n",
      "Epoch [533/1000], Training Loss: 0.8292, Test Loss: 76.3317\n",
      "Epoch [534/1000], Training Loss: 0.7146, Test Loss: 75.5891\n",
      "Epoch [535/1000], Training Loss: 0.5964, Test Loss: 75.5302\n",
      "Epoch [536/1000], Training Loss: 0.8426, Test Loss: 76.8254\n",
      "Epoch [537/1000], Training Loss: 2.1910, Test Loss: 73.8209\n",
      "Epoch [538/1000], Training Loss: 1.1528, Test Loss: 74.7189\n",
      "Epoch [539/1000], Training Loss: 1.1772, Test Loss: 74.7098\n",
      "Epoch [540/1000], Training Loss: 0.9631, Test Loss: 74.4830\n",
      "Epoch [541/1000], Training Loss: 0.8188, Test Loss: 75.0327\n",
      "Epoch [542/1000], Training Loss: 0.9158, Test Loss: 75.0660\n",
      "Epoch [543/1000], Training Loss: 0.8049, Test Loss: 76.0742\n",
      "Epoch [544/1000], Training Loss: 0.8151, Test Loss: 76.1159\n",
      "Epoch [545/1000], Training Loss: 0.9710, Test Loss: 76.3163\n",
      "Epoch [546/1000], Training Loss: 0.7177, Test Loss: 75.0625\n",
      "Epoch [547/1000], Training Loss: 1.8953, Test Loss: 75.8637\n",
      "Epoch [548/1000], Training Loss: 0.8457, Test Loss: 76.3193\n",
      "Epoch [549/1000], Training Loss: 2.2106, Test Loss: 75.4298\n",
      "Epoch [550/1000], Training Loss: 2.1807, Test Loss: 75.8845\n",
      "Epoch [551/1000], Training Loss: 1.0704, Test Loss: 73.7816\n",
      "Epoch [552/1000], Training Loss: 1.6742, Test Loss: 75.7110\n",
      "Epoch [553/1000], Training Loss: 1.8337, Test Loss: 77.1219\n",
      "Epoch [554/1000], Training Loss: 1.1465, Test Loss: 77.1717\n",
      "Epoch [555/1000], Training Loss: 1.7843, Test Loss: 74.6110\n",
      "Epoch [556/1000], Training Loss: 1.3818, Test Loss: 75.9314\n",
      "Epoch [557/1000], Training Loss: 1.8275, Test Loss: 76.6361\n",
      "Epoch [558/1000], Training Loss: 1.0994, Test Loss: 77.2676\n",
      "Epoch [559/1000], Training Loss: 0.5443, Test Loss: 76.3092\n",
      "Epoch [560/1000], Training Loss: 1.6424, Test Loss: 75.3312\n",
      "Epoch [561/1000], Training Loss: 2.7097, Test Loss: 76.1640\n",
      "Epoch [562/1000], Training Loss: 1.2437, Test Loss: 75.2707\n",
      "Epoch [563/1000], Training Loss: 0.8987, Test Loss: 73.3766\n",
      "Epoch [564/1000], Training Loss: 0.8991, Test Loss: 74.7762\n",
      "Epoch [565/1000], Training Loss: 1.0332, Test Loss: 76.0546\n",
      "Epoch [566/1000], Training Loss: 0.8837, Test Loss: 74.1532\n",
      "Epoch [567/1000], Training Loss: 0.9507, Test Loss: 75.7464\n",
      "Epoch [568/1000], Training Loss: 0.8555, Test Loss: 76.9682\n",
      "Epoch [569/1000], Training Loss: 0.7867, Test Loss: 76.6286\n",
      "Epoch [570/1000], Training Loss: 2.0440, Test Loss: 74.1591\n",
      "Epoch [571/1000], Training Loss: 1.3715, Test Loss: 73.7371\n",
      "Epoch [572/1000], Training Loss: 0.8050, Test Loss: 74.8450\n",
      "Epoch [573/1000], Training Loss: 2.1983, Test Loss: 76.2401\n",
      "Epoch [574/1000], Training Loss: 0.8261, Test Loss: 75.3640\n",
      "Epoch [575/1000], Training Loss: 0.7701, Test Loss: 75.8506\n",
      "Epoch [576/1000], Training Loss: 1.0237, Test Loss: 74.7906\n",
      "Epoch [577/1000], Training Loss: 0.7425, Test Loss: 74.8401\n",
      "Epoch [578/1000], Training Loss: 0.9406, Test Loss: 75.7772\n",
      "Epoch [579/1000], Training Loss: 1.0056, Test Loss: 75.9505\n",
      "Epoch [580/1000], Training Loss: 0.8031, Test Loss: 76.0085\n",
      "Epoch [581/1000], Training Loss: 1.7305, Test Loss: 75.5133\n",
      "Epoch [582/1000], Training Loss: 0.7764, Test Loss: 76.0859\n",
      "Epoch [583/1000], Training Loss: 0.9769, Test Loss: 74.8641\n",
      "Epoch [584/1000], Training Loss: 2.5512, Test Loss: 77.0508\n",
      "Epoch [585/1000], Training Loss: 0.7329, Test Loss: 75.4319\n",
      "Epoch [586/1000], Training Loss: 0.6154, Test Loss: 76.5726\n",
      "Epoch [587/1000], Training Loss: 1.6736, Test Loss: 75.9708\n",
      "Epoch [588/1000], Training Loss: 0.6832, Test Loss: 74.1771\n",
      "Epoch [589/1000], Training Loss: 0.8169, Test Loss: 74.7915\n",
      "Epoch [590/1000], Training Loss: 1.3094, Test Loss: 76.4442\n",
      "Epoch [591/1000], Training Loss: 0.9617, Test Loss: 75.6382\n",
      "Epoch [592/1000], Training Loss: 0.6702, Test Loss: 76.9720\n",
      "Epoch [593/1000], Training Loss: 0.9247, Test Loss: 76.6463\n",
      "Epoch [594/1000], Training Loss: 0.6327, Test Loss: 74.9889\n",
      "Epoch [595/1000], Training Loss: 0.9883, Test Loss: 76.2136\n",
      "Epoch [596/1000], Training Loss: 2.0987, Test Loss: 77.2836\n",
      "Epoch [597/1000], Training Loss: 1.4062, Test Loss: 74.5778\n",
      "Epoch [598/1000], Training Loss: 0.9749, Test Loss: 74.9093\n",
      "Epoch [599/1000], Training Loss: 0.8716, Test Loss: 75.5940\n",
      "Epoch [600/1000], Training Loss: 2.4264, Test Loss: 75.5086\n",
      "Epoch [601/1000], Training Loss: 0.7914, Test Loss: 76.4717\n",
      "Epoch [602/1000], Training Loss: 1.4555, Test Loss: 75.3585\n",
      "Epoch [603/1000], Training Loss: 0.8996, Test Loss: 74.8574\n",
      "Epoch [604/1000], Training Loss: 1.2753, Test Loss: 74.1288\n",
      "Epoch [605/1000], Training Loss: 0.6716, Test Loss: 76.5046\n",
      "Epoch [606/1000], Training Loss: 0.6412, Test Loss: 75.9862\n",
      "Epoch [607/1000], Training Loss: 1.8565, Test Loss: 76.0672\n",
      "Epoch [608/1000], Training Loss: 0.7235, Test Loss: 76.3234\n",
      "Epoch [609/1000], Training Loss: 1.2800, Test Loss: 75.1534\n",
      "Epoch [610/1000], Training Loss: 1.1925, Test Loss: 74.5877\n",
      "Epoch [611/1000], Training Loss: 1.7220, Test Loss: 75.0965\n",
      "Epoch [612/1000], Training Loss: 0.9035, Test Loss: 74.1109\n",
      "Epoch [613/1000], Training Loss: 1.1337, Test Loss: 76.5344\n",
      "Epoch [614/1000], Training Loss: 0.8983, Test Loss: 74.6714\n",
      "Epoch [615/1000], Training Loss: 0.7796, Test Loss: 75.1713\n",
      "Epoch [616/1000], Training Loss: 1.0591, Test Loss: 73.4746\n",
      "Epoch [617/1000], Training Loss: 0.9123, Test Loss: 76.3479\n",
      "Epoch [618/1000], Training Loss: 1.6071, Test Loss: 75.1772\n",
      "Epoch [619/1000], Training Loss: 0.7763, Test Loss: 77.1470\n",
      "Epoch [620/1000], Training Loss: 0.5545, Test Loss: 76.4920\n",
      "Epoch [621/1000], Training Loss: 1.1608, Test Loss: 74.9714\n",
      "Epoch [622/1000], Training Loss: 1.4136, Test Loss: 75.2602\n",
      "Epoch [623/1000], Training Loss: 0.8525, Test Loss: 74.7644\n",
      "Epoch [624/1000], Training Loss: 1.0261, Test Loss: 77.4032\n",
      "Epoch [625/1000], Training Loss: 1.0847, Test Loss: 77.7551\n",
      "Epoch [626/1000], Training Loss: 1.2122, Test Loss: 75.1609\n",
      "Epoch [627/1000], Training Loss: 0.8834, Test Loss: 75.5464\n",
      "Epoch [628/1000], Training Loss: 1.0439, Test Loss: 76.0547\n",
      "Epoch [629/1000], Training Loss: 0.8706, Test Loss: 74.6722\n",
      "Epoch [630/1000], Training Loss: 2.2975, Test Loss: 76.2186\n",
      "Epoch [631/1000], Training Loss: 0.8546, Test Loss: 75.9327\n",
      "Epoch [632/1000], Training Loss: 0.5501, Test Loss: 76.1397\n",
      "Epoch [633/1000], Training Loss: 1.0103, Test Loss: 75.4670\n",
      "Epoch [634/1000], Training Loss: 1.8220, Test Loss: 78.5206\n",
      "Epoch [635/1000], Training Loss: 1.2108, Test Loss: 78.1825\n",
      "Epoch [636/1000], Training Loss: 1.5080, Test Loss: 75.8510\n",
      "Epoch [637/1000], Training Loss: 0.9608, Test Loss: 75.6420\n",
      "Epoch [638/1000], Training Loss: 0.6178, Test Loss: 76.4993\n",
      "Epoch [639/1000], Training Loss: 0.8168, Test Loss: 75.6646\n",
      "Epoch [640/1000], Training Loss: 1.2554, Test Loss: 75.7458\n",
      "Epoch [641/1000], Training Loss: 0.7622, Test Loss: 75.5804\n",
      "Epoch [642/1000], Training Loss: 1.6083, Test Loss: 77.8810\n",
      "Epoch [643/1000], Training Loss: 1.1140, Test Loss: 75.4808\n",
      "Epoch [644/1000], Training Loss: 1.1909, Test Loss: 76.7224\n",
      "Epoch [645/1000], Training Loss: 1.4436, Test Loss: 74.9109\n",
      "Epoch [646/1000], Training Loss: 1.2798, Test Loss: 75.1646\n",
      "Epoch [647/1000], Training Loss: 0.8208, Test Loss: 75.4560\n",
      "Epoch [648/1000], Training Loss: 1.9629, Test Loss: 73.8160\n",
      "Epoch [649/1000], Training Loss: 0.8788, Test Loss: 74.1825\n",
      "Epoch [650/1000], Training Loss: 1.0111, Test Loss: 74.8200\n",
      "Epoch [651/1000], Training Loss: 0.6071, Test Loss: 75.4012\n",
      "Epoch [652/1000], Training Loss: 0.6625, Test Loss: 75.2067\n",
      "Epoch [653/1000], Training Loss: 0.5616, Test Loss: 76.6976\n",
      "Epoch [654/1000], Training Loss: 0.9788, Test Loss: 74.7537\n",
      "Epoch [655/1000], Training Loss: 2.1239, Test Loss: 76.3247\n",
      "Epoch [656/1000], Training Loss: 1.6330, Test Loss: 74.5655\n",
      "Epoch [657/1000], Training Loss: 0.6580, Test Loss: 77.3765\n",
      "Epoch [658/1000], Training Loss: 0.6463, Test Loss: 76.2764\n",
      "Epoch [659/1000], Training Loss: 1.0880, Test Loss: 77.2314\n",
      "Epoch [660/1000], Training Loss: 1.0315, Test Loss: 75.0993\n",
      "Epoch [661/1000], Training Loss: 0.9029, Test Loss: 76.6627\n",
      "Epoch [662/1000], Training Loss: 0.8052, Test Loss: 73.9206\n",
      "Epoch [663/1000], Training Loss: 0.6219, Test Loss: 75.2463\n",
      "Epoch [664/1000], Training Loss: 1.0874, Test Loss: 76.2328\n",
      "Epoch [665/1000], Training Loss: 1.6426, Test Loss: 74.2032\n",
      "Epoch [666/1000], Training Loss: 1.1266, Test Loss: 76.4107\n",
      "Epoch [667/1000], Training Loss: 1.0178, Test Loss: 76.4279\n",
      "Epoch [668/1000], Training Loss: 0.8827, Test Loss: 74.8893\n",
      "Epoch [669/1000], Training Loss: 1.3963, Test Loss: 75.5562\n",
      "Epoch [670/1000], Training Loss: 1.2053, Test Loss: 75.8202\n",
      "Epoch [671/1000], Training Loss: 2.1291, Test Loss: 77.1790\n",
      "Epoch [672/1000], Training Loss: 1.1970, Test Loss: 78.3895\n",
      "Epoch [673/1000], Training Loss: 1.1450, Test Loss: 76.4209\n",
      "Epoch [674/1000], Training Loss: 0.9188, Test Loss: 75.9417\n",
      "Epoch [675/1000], Training Loss: 0.9496, Test Loss: 76.7861\n",
      "Epoch [676/1000], Training Loss: 0.9200, Test Loss: 74.7230\n",
      "Epoch [677/1000], Training Loss: 0.9883, Test Loss: 74.5500\n",
      "Epoch [678/1000], Training Loss: 1.7692, Test Loss: 75.6189\n",
      "Epoch [679/1000], Training Loss: 1.0507, Test Loss: 75.9098\n",
      "Epoch [680/1000], Training Loss: 0.5755, Test Loss: 76.2494\n",
      "Epoch [681/1000], Training Loss: 1.3844, Test Loss: 76.0520\n",
      "Epoch [682/1000], Training Loss: 0.5011, Test Loss: 75.4260\n",
      "Epoch [683/1000], Training Loss: 1.4274, Test Loss: 76.4004\n",
      "Epoch [684/1000], Training Loss: 0.7865, Test Loss: 76.2148\n",
      "Epoch [685/1000], Training Loss: 2.3556, Test Loss: 74.3259\n",
      "Epoch [686/1000], Training Loss: 1.1918, Test Loss: 75.9123\n",
      "Epoch [687/1000], Training Loss: 1.3128, Test Loss: 75.4051\n",
      "Epoch [688/1000], Training Loss: 0.7249, Test Loss: 74.8028\n",
      "Epoch [689/1000], Training Loss: 0.8661, Test Loss: 75.5897\n",
      "Epoch [690/1000], Training Loss: 0.7344, Test Loss: 75.5608\n",
      "Epoch [691/1000], Training Loss: 1.1501, Test Loss: 76.2014\n",
      "Epoch [692/1000], Training Loss: 0.6310, Test Loss: 78.0346\n",
      "Epoch [693/1000], Training Loss: 0.4703, Test Loss: 75.5624\n",
      "Epoch [694/1000], Training Loss: 1.9500, Test Loss: 74.7773\n",
      "Epoch [695/1000], Training Loss: 0.6528, Test Loss: 77.4789\n",
      "Epoch [696/1000], Training Loss: 0.8505, Test Loss: 75.6109\n",
      "Epoch [697/1000], Training Loss: 0.9713, Test Loss: 76.3989\n",
      "Epoch [698/1000], Training Loss: 1.2151, Test Loss: 76.3984\n",
      "Epoch [699/1000], Training Loss: 1.0892, Test Loss: 75.5430\n",
      "Epoch [700/1000], Training Loss: 2.0845, Test Loss: 76.5945\n",
      "Epoch [701/1000], Training Loss: 0.7199, Test Loss: 76.3667\n",
      "Epoch [702/1000], Training Loss: 0.5500, Test Loss: 75.7126\n",
      "Epoch [703/1000], Training Loss: 0.9197, Test Loss: 75.7846\n",
      "Epoch [704/1000], Training Loss: 1.2022, Test Loss: 75.5701\n",
      "Epoch [705/1000], Training Loss: 1.2555, Test Loss: 76.6838\n",
      "Epoch [706/1000], Training Loss: 1.8462, Test Loss: 75.4957\n",
      "Epoch [707/1000], Training Loss: 0.9530, Test Loss: 75.9166\n",
      "Epoch [708/1000], Training Loss: 1.2918, Test Loss: 75.0874\n",
      "Epoch [709/1000], Training Loss: 1.2513, Test Loss: 75.1630\n",
      "Epoch [710/1000], Training Loss: 1.1728, Test Loss: 74.2508\n",
      "Epoch [711/1000], Training Loss: 1.0806, Test Loss: 75.0814\n",
      "Epoch [712/1000], Training Loss: 1.1713, Test Loss: 74.7240\n",
      "Epoch [713/1000], Training Loss: 0.6438, Test Loss: 76.0642\n",
      "Epoch [714/1000], Training Loss: 1.8127, Test Loss: 75.4992\n",
      "Epoch [715/1000], Training Loss: 0.8211, Test Loss: 75.0241\n",
      "Epoch [716/1000], Training Loss: 2.0338, Test Loss: 77.7990\n",
      "Epoch [717/1000], Training Loss: 1.1116, Test Loss: 75.0344\n",
      "Epoch [718/1000], Training Loss: 0.8696, Test Loss: 75.1335\n",
      "Epoch [719/1000], Training Loss: 3.7938, Test Loss: 75.6009\n",
      "Epoch [720/1000], Training Loss: 1.3314, Test Loss: 75.1424\n",
      "Epoch [721/1000], Training Loss: 0.6015, Test Loss: 75.6174\n",
      "Epoch [722/1000], Training Loss: 0.7811, Test Loss: 75.7217\n",
      "Epoch [723/1000], Training Loss: 0.7710, Test Loss: 76.0794\n",
      "Epoch [724/1000], Training Loss: 0.9820, Test Loss: 76.7231\n",
      "Epoch [725/1000], Training Loss: 1.2512, Test Loss: 75.2852\n",
      "Epoch [726/1000], Training Loss: 0.8422, Test Loss: 75.6120\n",
      "Epoch [727/1000], Training Loss: 0.7646, Test Loss: 75.3272\n",
      "Epoch [728/1000], Training Loss: 1.1316, Test Loss: 73.5004\n",
      "Epoch [729/1000], Training Loss: 0.7529, Test Loss: 76.4195\n",
      "Epoch [730/1000], Training Loss: 1.8257, Test Loss: 74.5032\n",
      "Epoch [731/1000], Training Loss: 1.2539, Test Loss: 75.7940\n",
      "Epoch [732/1000], Training Loss: 1.0874, Test Loss: 75.5102\n",
      "Epoch [733/1000], Training Loss: 0.8623, Test Loss: 73.8361\n",
      "Epoch [734/1000], Training Loss: 1.1453, Test Loss: 76.5077\n",
      "Epoch [735/1000], Training Loss: 0.8552, Test Loss: 75.6003\n",
      "Epoch [736/1000], Training Loss: 0.8219, Test Loss: 74.6847\n",
      "Epoch [737/1000], Training Loss: 1.5363, Test Loss: 75.7382\n",
      "Epoch [738/1000], Training Loss: 2.0966, Test Loss: 76.3649\n",
      "Epoch [739/1000], Training Loss: 0.7588, Test Loss: 76.3570\n",
      "Epoch [740/1000], Training Loss: 1.2198, Test Loss: 75.5902\n",
      "Epoch [741/1000], Training Loss: 0.7823, Test Loss: 75.9710\n",
      "Epoch [742/1000], Training Loss: 0.7470, Test Loss: 76.4383\n",
      "Epoch [743/1000], Training Loss: 0.8349, Test Loss: 75.7268\n",
      "Epoch [744/1000], Training Loss: 0.8605, Test Loss: 75.1199\n",
      "Epoch [745/1000], Training Loss: 0.9683, Test Loss: 78.3025\n",
      "Epoch [746/1000], Training Loss: 1.1458, Test Loss: 75.7665\n",
      "Epoch [747/1000], Training Loss: 0.7957, Test Loss: 76.1373\n",
      "Epoch [748/1000], Training Loss: 1.6976, Test Loss: 75.8100\n",
      "Epoch [749/1000], Training Loss: 0.6889, Test Loss: 75.4238\n",
      "Epoch [750/1000], Training Loss: 0.6655, Test Loss: 75.5857\n",
      "Epoch [751/1000], Training Loss: 0.6299, Test Loss: 75.7398\n",
      "Epoch [752/1000], Training Loss: 0.7843, Test Loss: 75.6673\n",
      "Epoch [753/1000], Training Loss: 1.6753, Test Loss: 77.1686\n",
      "Epoch [754/1000], Training Loss: 0.7359, Test Loss: 78.1309\n",
      "Epoch [755/1000], Training Loss: 1.4134, Test Loss: 75.7190\n",
      "Epoch [756/1000], Training Loss: 0.7046, Test Loss: 74.9758\n",
      "Epoch [757/1000], Training Loss: 2.0112, Test Loss: 75.6609\n",
      "Epoch [758/1000], Training Loss: 0.6958, Test Loss: 74.9314\n",
      "Epoch [759/1000], Training Loss: 0.9378, Test Loss: 76.2997\n",
      "Epoch [760/1000], Training Loss: 0.9473, Test Loss: 75.0624\n",
      "Epoch [761/1000], Training Loss: 1.1222, Test Loss: 75.6160\n",
      "Epoch [762/1000], Training Loss: 1.2530, Test Loss: 74.8731\n",
      "Epoch [763/1000], Training Loss: 2.6431, Test Loss: 76.1598\n",
      "Epoch [764/1000], Training Loss: 1.6230, Test Loss: 74.7331\n",
      "Epoch [765/1000], Training Loss: 1.3663, Test Loss: 75.6910\n",
      "Epoch [766/1000], Training Loss: 0.9919, Test Loss: 74.1082\n",
      "Epoch [767/1000], Training Loss: 1.2550, Test Loss: 76.6434\n",
      "Epoch [768/1000], Training Loss: 1.1299, Test Loss: 75.4957\n",
      "Epoch [769/1000], Training Loss: 0.7648, Test Loss: 75.0319\n",
      "Epoch [770/1000], Training Loss: 1.7164, Test Loss: 75.6037\n",
      "Epoch [771/1000], Training Loss: 0.7877, Test Loss: 75.0321\n",
      "Epoch [772/1000], Training Loss: 1.0718, Test Loss: 74.6975\n",
      "Epoch [773/1000], Training Loss: 0.6947, Test Loss: 76.1509\n",
      "Epoch [774/1000], Training Loss: 1.1028, Test Loss: 75.9493\n",
      "Epoch [775/1000], Training Loss: 1.7158, Test Loss: 76.6920\n",
      "Epoch [776/1000], Training Loss: 0.9624, Test Loss: 75.6747\n",
      "Epoch [777/1000], Training Loss: 0.9333, Test Loss: 77.2392\n",
      "Epoch [778/1000], Training Loss: 1.5299, Test Loss: 76.8485\n",
      "Epoch [779/1000], Training Loss: 1.0577, Test Loss: 76.1346\n",
      "Epoch [780/1000], Training Loss: 1.6764, Test Loss: 75.1109\n",
      "Epoch [781/1000], Training Loss: 1.8949, Test Loss: 74.0109\n",
      "Epoch [782/1000], Training Loss: 0.7303, Test Loss: 74.4902\n",
      "Epoch [783/1000], Training Loss: 1.0377, Test Loss: 74.3503\n",
      "Epoch [784/1000], Training Loss: 1.3441, Test Loss: 76.0042\n",
      "Epoch [785/1000], Training Loss: 0.6788, Test Loss: 75.2271\n",
      "Epoch [786/1000], Training Loss: 0.8867, Test Loss: 75.5539\n",
      "Epoch [787/1000], Training Loss: 0.5957, Test Loss: 74.7677\n",
      "Epoch [788/1000], Training Loss: 0.9156, Test Loss: 76.6524\n",
      "Epoch [789/1000], Training Loss: 0.8916, Test Loss: 76.6107\n",
      "Epoch [790/1000], Training Loss: 1.3178, Test Loss: 75.4661\n",
      "Epoch [791/1000], Training Loss: 1.6498, Test Loss: 75.2309\n",
      "Epoch [792/1000], Training Loss: 0.6934, Test Loss: 76.2886\n",
      "Epoch [793/1000], Training Loss: 0.8820, Test Loss: 76.1108\n",
      "Epoch [794/1000], Training Loss: 0.9973, Test Loss: 76.5782\n",
      "Epoch [795/1000], Training Loss: 1.5315, Test Loss: 76.1872\n",
      "Epoch [796/1000], Training Loss: 2.3026, Test Loss: 76.3749\n",
      "Epoch [797/1000], Training Loss: 0.8604, Test Loss: 76.2875\n",
      "Epoch [798/1000], Training Loss: 1.7023, Test Loss: 76.8816\n",
      "Epoch [799/1000], Training Loss: 1.5583, Test Loss: 75.7835\n",
      "Epoch [800/1000], Training Loss: 1.3766, Test Loss: 76.7310\n",
      "Epoch [801/1000], Training Loss: 1.1285, Test Loss: 77.7494\n",
      "Epoch [802/1000], Training Loss: 1.0400, Test Loss: 75.1902\n",
      "Epoch [803/1000], Training Loss: 1.1569, Test Loss: 76.6409\n",
      "Epoch [804/1000], Training Loss: 0.8062, Test Loss: 75.2310\n",
      "Epoch [805/1000], Training Loss: 1.3609, Test Loss: 76.4525\n",
      "Epoch [806/1000], Training Loss: 0.8543, Test Loss: 76.6057\n",
      "Epoch [807/1000], Training Loss: 1.0970, Test Loss: 75.2605\n",
      "Epoch [808/1000], Training Loss: 1.2177, Test Loss: 75.2955\n",
      "Epoch [809/1000], Training Loss: 1.4359, Test Loss: 74.8806\n",
      "Epoch [810/1000], Training Loss: 2.6423, Test Loss: 76.0936\n",
      "Epoch [811/1000], Training Loss: 0.9704, Test Loss: 75.8674\n",
      "Epoch [812/1000], Training Loss: 1.2254, Test Loss: 74.5899\n",
      "Epoch [813/1000], Training Loss: 1.2079, Test Loss: 76.9863\n",
      "Epoch [814/1000], Training Loss: 2.3697, Test Loss: 75.3510\n",
      "Epoch [815/1000], Training Loss: 1.3136, Test Loss: 76.7705\n",
      "Epoch [816/1000], Training Loss: 1.3421, Test Loss: 75.5477\n",
      "Epoch [817/1000], Training Loss: 0.6518, Test Loss: 74.3221\n",
      "Epoch [818/1000], Training Loss: 2.3873, Test Loss: 75.4273\n",
      "Epoch [819/1000], Training Loss: 0.6380, Test Loss: 75.4499\n",
      "Epoch [820/1000], Training Loss: 0.9263, Test Loss: 76.3773\n",
      "Epoch [821/1000], Training Loss: 0.8438, Test Loss: 76.0180\n",
      "Epoch [822/1000], Training Loss: 1.1071, Test Loss: 76.2538\n",
      "Epoch [823/1000], Training Loss: 1.0061, Test Loss: 75.7513\n",
      "Epoch [824/1000], Training Loss: 1.1073, Test Loss: 76.3774\n",
      "Epoch [825/1000], Training Loss: 1.2852, Test Loss: 76.4759\n",
      "Epoch [826/1000], Training Loss: 0.5756, Test Loss: 75.6334\n",
      "Epoch [827/1000], Training Loss: 1.0323, Test Loss: 76.3008\n",
      "Epoch [828/1000], Training Loss: 0.7012, Test Loss: 77.2120\n",
      "Epoch [829/1000], Training Loss: 0.7713, Test Loss: 74.8244\n",
      "Epoch [830/1000], Training Loss: 1.6897, Test Loss: 74.7146\n",
      "Epoch [831/1000], Training Loss: 0.8504, Test Loss: 75.4556\n",
      "Epoch [832/1000], Training Loss: 2.0859, Test Loss: 76.1803\n",
      "Epoch [833/1000], Training Loss: 0.8720, Test Loss: 75.1719\n",
      "Epoch [834/1000], Training Loss: 2.5841, Test Loss: 76.4641\n",
      "Epoch [835/1000], Training Loss: 2.9469, Test Loss: 75.7516\n",
      "Epoch [836/1000], Training Loss: 0.8774, Test Loss: 75.4680\n",
      "Epoch [837/1000], Training Loss: 0.6709, Test Loss: 75.2151\n",
      "Epoch [838/1000], Training Loss: 0.7015, Test Loss: 74.7027\n",
      "Epoch [839/1000], Training Loss: 1.1618, Test Loss: 75.7307\n",
      "Epoch [840/1000], Training Loss: 0.8938, Test Loss: 76.1611\n",
      "Epoch [841/1000], Training Loss: 2.3299, Test Loss: 77.1881\n",
      "Epoch [842/1000], Training Loss: 1.0016, Test Loss: 74.8853\n",
      "Epoch [843/1000], Training Loss: 0.4923, Test Loss: 75.8543\n",
      "Epoch [844/1000], Training Loss: 1.1222, Test Loss: 74.8022\n",
      "Epoch [845/1000], Training Loss: 1.2594, Test Loss: 75.1583\n",
      "Epoch [846/1000], Training Loss: 1.2808, Test Loss: 77.0229\n",
      "Epoch [847/1000], Training Loss: 2.6358, Test Loss: 74.8901\n",
      "Epoch [848/1000], Training Loss: 1.5423, Test Loss: 76.5579\n",
      "Epoch [849/1000], Training Loss: 0.9964, Test Loss: 75.7634\n",
      "Epoch [850/1000], Training Loss: 1.8817, Test Loss: 76.5996\n",
      "Epoch [851/1000], Training Loss: 0.8569, Test Loss: 77.2045\n",
      "Epoch [852/1000], Training Loss: 0.5730, Test Loss: 75.7148\n",
      "Epoch [853/1000], Training Loss: 1.0039, Test Loss: 74.0270\n",
      "Epoch [854/1000], Training Loss: 0.5644, Test Loss: 75.8525\n",
      "Epoch [855/1000], Training Loss: 0.7790, Test Loss: 75.7471\n",
      "Epoch [856/1000], Training Loss: 1.7686, Test Loss: 77.6073\n",
      "Epoch [857/1000], Training Loss: 1.3339, Test Loss: 76.1866\n",
      "Epoch [858/1000], Training Loss: 0.8128, Test Loss: 76.6375\n",
      "Epoch [859/1000], Training Loss: 0.5576, Test Loss: 75.6771\n",
      "Epoch [860/1000], Training Loss: 1.1324, Test Loss: 76.5108\n",
      "Epoch [861/1000], Training Loss: 1.0027, Test Loss: 77.7256\n",
      "Epoch [862/1000], Training Loss: 0.8826, Test Loss: 75.3448\n",
      "Epoch [863/1000], Training Loss: 0.8079, Test Loss: 76.0816\n",
      "Epoch [864/1000], Training Loss: 1.0130, Test Loss: 74.9367\n",
      "Epoch [865/1000], Training Loss: 1.3293, Test Loss: 75.7813\n",
      "Epoch [866/1000], Training Loss: 0.6626, Test Loss: 75.0880\n",
      "Epoch [867/1000], Training Loss: 1.4808, Test Loss: 76.0399\n",
      "Epoch [868/1000], Training Loss: 0.9742, Test Loss: 74.8584\n",
      "Epoch [869/1000], Training Loss: 1.9141, Test Loss: 76.2455\n",
      "Epoch [870/1000], Training Loss: 0.9753, Test Loss: 75.5255\n",
      "Epoch [871/1000], Training Loss: 1.0399, Test Loss: 76.3152\n",
      "Epoch [872/1000], Training Loss: 0.7983, Test Loss: 75.8712\n",
      "Epoch [873/1000], Training Loss: 0.9345, Test Loss: 76.7469\n",
      "Epoch [874/1000], Training Loss: 1.2361, Test Loss: 75.2610\n",
      "Epoch [875/1000], Training Loss: 1.3590, Test Loss: 76.3581\n",
      "Epoch [876/1000], Training Loss: 1.2706, Test Loss: 76.8112\n",
      "Epoch [877/1000], Training Loss: 0.8420, Test Loss: 75.8764\n",
      "Epoch [878/1000], Training Loss: 1.2049, Test Loss: 76.2823\n",
      "Epoch [879/1000], Training Loss: 0.9646, Test Loss: 74.3154\n",
      "Epoch [880/1000], Training Loss: 0.7860, Test Loss: 76.9126\n",
      "Epoch [881/1000], Training Loss: 0.7985, Test Loss: 76.0896\n",
      "Epoch [882/1000], Training Loss: 1.8294, Test Loss: 75.6109\n",
      "Epoch [883/1000], Training Loss: 0.7710, Test Loss: 76.4279\n",
      "Epoch [884/1000], Training Loss: 1.3167, Test Loss: 74.8905\n",
      "Epoch [885/1000], Training Loss: 0.6592, Test Loss: 77.4213\n",
      "Epoch [886/1000], Training Loss: 1.4215, Test Loss: 75.7360\n",
      "Epoch [887/1000], Training Loss: 0.5764, Test Loss: 76.3269\n",
      "Epoch [888/1000], Training Loss: 1.2067, Test Loss: 75.4509\n",
      "Epoch [889/1000], Training Loss: 1.3907, Test Loss: 75.9728\n",
      "Epoch [890/1000], Training Loss: 0.6583, Test Loss: 75.4801\n",
      "Epoch [891/1000], Training Loss: 0.9897, Test Loss: 77.1721\n",
      "Epoch [892/1000], Training Loss: 0.9206, Test Loss: 75.4337\n",
      "Epoch [893/1000], Training Loss: 2.1886, Test Loss: 75.6723\n",
      "Epoch [894/1000], Training Loss: 0.9088, Test Loss: 74.2763\n",
      "Epoch [895/1000], Training Loss: 0.7509, Test Loss: 76.2508\n",
      "Epoch [896/1000], Training Loss: 0.7306, Test Loss: 75.4729\n",
      "Epoch [897/1000], Training Loss: 0.6948, Test Loss: 75.2443\n",
      "Epoch [898/1000], Training Loss: 0.6836, Test Loss: 76.0605\n",
      "Epoch [899/1000], Training Loss: 0.8274, Test Loss: 75.0656\n",
      "Epoch [900/1000], Training Loss: 1.2944, Test Loss: 74.3144\n",
      "Epoch [901/1000], Training Loss: 1.8843, Test Loss: 74.1878\n",
      "Epoch [902/1000], Training Loss: 1.1845, Test Loss: 76.7862\n",
      "Epoch [903/1000], Training Loss: 0.6543, Test Loss: 76.8484\n",
      "Epoch [904/1000], Training Loss: 0.7330, Test Loss: 73.6356\n",
      "Epoch [905/1000], Training Loss: 0.9365, Test Loss: 74.9513\n",
      "Epoch [906/1000], Training Loss: 1.3040, Test Loss: 76.5638\n",
      "Epoch [907/1000], Training Loss: 0.6168, Test Loss: 77.6649\n",
      "Epoch [908/1000], Training Loss: 0.8568, Test Loss: 75.7037\n",
      "Epoch [909/1000], Training Loss: 1.1069, Test Loss: 74.7091\n",
      "Epoch [910/1000], Training Loss: 0.5781, Test Loss: 76.3215\n",
      "Epoch [911/1000], Training Loss: 1.0187, Test Loss: 76.2766\n",
      "Epoch [912/1000], Training Loss: 1.4040, Test Loss: 75.4854\n",
      "Epoch [913/1000], Training Loss: 0.8083, Test Loss: 76.1921\n",
      "Epoch [914/1000], Training Loss: 0.9487, Test Loss: 75.7152\n",
      "Epoch [915/1000], Training Loss: 0.7070, Test Loss: 77.3791\n",
      "Epoch [916/1000], Training Loss: 0.9220, Test Loss: 75.8367\n",
      "Epoch [917/1000], Training Loss: 0.7364, Test Loss: 75.2628\n",
      "Epoch [918/1000], Training Loss: 0.8888, Test Loss: 76.7941\n",
      "Epoch [919/1000], Training Loss: 1.1729, Test Loss: 75.4953\n",
      "Epoch [920/1000], Training Loss: 0.7377, Test Loss: 76.1867\n",
      "Epoch [921/1000], Training Loss: 1.0230, Test Loss: 77.0653\n",
      "Epoch [922/1000], Training Loss: 0.5609, Test Loss: 75.5288\n",
      "Epoch [923/1000], Training Loss: 1.2586, Test Loss: 76.4477\n",
      "Epoch [924/1000], Training Loss: 1.0518, Test Loss: 76.5301\n",
      "Epoch [925/1000], Training Loss: 0.6742, Test Loss: 76.4578\n",
      "Epoch [926/1000], Training Loss: 1.4454, Test Loss: 75.4977\n",
      "Epoch [927/1000], Training Loss: 0.5834, Test Loss: 74.4531\n",
      "Epoch [928/1000], Training Loss: 0.8776, Test Loss: 75.8389\n",
      "Epoch [929/1000], Training Loss: 1.5921, Test Loss: 75.6652\n",
      "Epoch [930/1000], Training Loss: 0.8890, Test Loss: 77.0858\n",
      "Epoch [931/1000], Training Loss: 1.1306, Test Loss: 74.5266\n",
      "Epoch [932/1000], Training Loss: 0.8674, Test Loss: 75.3382\n",
      "Epoch [933/1000], Training Loss: 1.1075, Test Loss: 76.1814\n",
      "Epoch [934/1000], Training Loss: 0.6757, Test Loss: 74.8417\n",
      "Epoch [935/1000], Training Loss: 1.1858, Test Loss: 76.7476\n",
      "Epoch [936/1000], Training Loss: 1.1331, Test Loss: 76.3358\n",
      "Epoch [937/1000], Training Loss: 0.6296, Test Loss: 76.1639\n",
      "Epoch [938/1000], Training Loss: 0.9546, Test Loss: 74.9643\n",
      "Epoch [939/1000], Training Loss: 1.1025, Test Loss: 75.5274\n",
      "Epoch [940/1000], Training Loss: 1.6603, Test Loss: 76.8890\n",
      "Epoch [941/1000], Training Loss: 0.6576, Test Loss: 76.0352\n",
      "Epoch [942/1000], Training Loss: 0.8395, Test Loss: 76.5351\n",
      "Epoch [943/1000], Training Loss: 0.7048, Test Loss: 74.7460\n",
      "Epoch [944/1000], Training Loss: 1.0117, Test Loss: 76.0267\n",
      "Epoch [945/1000], Training Loss: 0.8924, Test Loss: 76.3211\n",
      "Epoch [946/1000], Training Loss: 0.6868, Test Loss: 77.7267\n",
      "Epoch [947/1000], Training Loss: 1.0615, Test Loss: 76.2240\n",
      "Epoch [948/1000], Training Loss: 0.6464, Test Loss: 76.1494\n",
      "Epoch [949/1000], Training Loss: 0.9502, Test Loss: 74.9904\n",
      "Epoch [950/1000], Training Loss: 0.7061, Test Loss: 76.8167\n",
      "Epoch [951/1000], Training Loss: 1.2284, Test Loss: 75.9691\n",
      "Epoch [952/1000], Training Loss: 2.4062, Test Loss: 75.1369\n",
      "Epoch [953/1000], Training Loss: 0.9571, Test Loss: 75.1535\n",
      "Epoch [954/1000], Training Loss: 0.9573, Test Loss: 74.7800\n",
      "Epoch [955/1000], Training Loss: 0.8503, Test Loss: 75.2157\n",
      "Epoch [956/1000], Training Loss: 1.5965, Test Loss: 75.3252\n",
      "Epoch [957/1000], Training Loss: 1.5443, Test Loss: 76.1839\n",
      "Epoch [958/1000], Training Loss: 1.1039, Test Loss: 76.0382\n",
      "Epoch [959/1000], Training Loss: 0.9453, Test Loss: 73.5896\n",
      "Epoch [960/1000], Training Loss: 2.4003, Test Loss: 75.8706\n",
      "Epoch [961/1000], Training Loss: 1.0119, Test Loss: 76.5534\n",
      "Epoch [962/1000], Training Loss: 0.7883, Test Loss: 76.6442\n",
      "Epoch [963/1000], Training Loss: 1.2577, Test Loss: 75.0025\n",
      "Epoch [964/1000], Training Loss: 0.9915, Test Loss: 75.5421\n",
      "Epoch [965/1000], Training Loss: 0.8864, Test Loss: 76.1420\n",
      "Epoch [966/1000], Training Loss: 0.8541, Test Loss: 74.8169\n",
      "Epoch [967/1000], Training Loss: 0.9473, Test Loss: 76.2472\n",
      "Epoch [968/1000], Training Loss: 2.1449, Test Loss: 76.9435\n",
      "Epoch [969/1000], Training Loss: 1.1380, Test Loss: 74.0909\n",
      "Epoch [970/1000], Training Loss: 0.6054, Test Loss: 75.4123\n",
      "Epoch [971/1000], Training Loss: 0.6395, Test Loss: 77.6687\n",
      "Epoch [972/1000], Training Loss: 0.9324, Test Loss: 77.3823\n",
      "Epoch [973/1000], Training Loss: 1.1455, Test Loss: 74.3618\n",
      "Epoch [974/1000], Training Loss: 0.9292, Test Loss: 75.5000\n",
      "Epoch [975/1000], Training Loss: 0.7017, Test Loss: 76.2166\n",
      "Epoch [976/1000], Training Loss: 0.8418, Test Loss: 75.9711\n",
      "Epoch [977/1000], Training Loss: 1.9830, Test Loss: 76.0709\n",
      "Epoch [978/1000], Training Loss: 0.6900, Test Loss: 77.2377\n",
      "Epoch [979/1000], Training Loss: 0.8402, Test Loss: 75.4745\n",
      "Epoch [980/1000], Training Loss: 1.0855, Test Loss: 75.5973\n",
      "Epoch [981/1000], Training Loss: 0.9574, Test Loss: 75.8771\n",
      "Epoch [982/1000], Training Loss: 1.9052, Test Loss: 75.6127\n",
      "Epoch [983/1000], Training Loss: 1.4443, Test Loss: 77.3635\n",
      "Epoch [984/1000], Training Loss: 1.6224, Test Loss: 74.5777\n",
      "Epoch [985/1000], Training Loss: 0.7862, Test Loss: 75.1527\n",
      "Epoch [986/1000], Training Loss: 1.1618, Test Loss: 73.2907\n",
      "Epoch [987/1000], Training Loss: 0.7944, Test Loss: 76.8335\n",
      "Epoch [988/1000], Training Loss: 1.0127, Test Loss: 75.9339\n",
      "Epoch [989/1000], Training Loss: 0.6342, Test Loss: 74.9429\n",
      "Epoch [990/1000], Training Loss: 0.8500, Test Loss: 76.3510\n",
      "Epoch [991/1000], Training Loss: 0.7090, Test Loss: 78.2718\n",
      "Epoch [992/1000], Training Loss: 0.6625, Test Loss: 75.3731\n",
      "Epoch [993/1000], Training Loss: 3.3556, Test Loss: 75.0626\n",
      "Epoch [994/1000], Training Loss: 0.9539, Test Loss: 76.5354\n",
      "Epoch [995/1000], Training Loss: 1.3985, Test Loss: 76.2519\n",
      "Epoch [996/1000], Training Loss: 0.9328, Test Loss: 76.7907\n",
      "Epoch [997/1000], Training Loss: 1.5763, Test Loss: 76.7754\n",
      "Epoch [998/1000], Training Loss: 2.4260, Test Loss: 77.2397\n",
      "Epoch [999/1000], Training Loss: 0.9710, Test Loss: 75.9961\n",
      "Epoch [1000/1000], Training Loss: 1.0304, Test Loss: 75.8329\n",
      "Finished Training and Evaluation\n",
      "CPU times: total: 54min 36s\n",
      "Wall time: 55min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train_and_evaluate_model(model, train_dataloader, test_dataloader, epochs, learning_rate):\n",
    "    # 定义损失函数\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # 定义优化器和学习率衰减\n",
    "    l2_lambda = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)  # 每100个epoch学习率乘以0.1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch < 500:lambda_weight=0.000001\n",
    "        else :lambda_weight=0.00001\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs_2d, inputs_3d, inputs_1d, targets in train_dataloader:\n",
    "            inputs_2d, inputs_3d, inputs_1d, targets = inputs_2d.to(device), inputs_3d.to(device), inputs_1d.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_2d, inputs_3d, inputs_1d).squeeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            weight_reg = lambda_weight * (model.weight_2d.norm(2) + model.weight_3d.norm(2) + model.weight_1d.norm(2))\n",
    "            loss += weight_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "\n",
    "        # 测试阶段\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs_2d, inputs_3d, inputs_1d, targets in test_dataloader:\n",
    "                inputs_2d, inputs_3d, inputs_1d, targets = inputs_2d.to(device), inputs_3d.to(device), inputs_1d.to(device), targets.to(device)\n",
    "                outputs = model(inputs_2d, inputs_3d, inputs_1d).squeeze(1)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = running_test_loss / len(test_dataloader)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Finished Training and Evaluation')\n",
    "\n",
    "# 用您的模型和数据初始化函数\n",
    "model = SimplifiedConvNet().to(device)\n",
    "train_and_evaluate_model(model, train_dataloader, test_dataloader, epochs=1000, learning_rate=0.01)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T10:58:07.478353500Z",
     "start_time": "2023-12-31T10:02:34.938517800Z"
    }
   },
   "id": "c3bf45832ead9f9e"
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "261\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T01:25:36.791996500Z",
     "start_time": "2023-12-29T01:25:36.759998100Z"
    }
   },
   "id": "b8379642a782f933"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('./runs')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T11:33:03.985601700Z",
     "start_time": "2023-12-28T11:33:03.850598Z"
    }
   },
   "id": "dcbf027301640b0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writer.add_scalar('mAP', mAP, epoch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1723f00b9fcc75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 可视化部分"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ecd4f9c7adfb10"
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            800\n",
      "├─BatchNorm2d: 1-2                       64\n",
      "├─MaxPool2d: 1-3                         --\n",
      "├─Conv2d: 1-4                            2,112\n",
      "├─BatchNorm2d: 1-5                       128\n",
      "├─MaxPool2d: 1-6                         --\n",
      "├─Conv3d: 1-7                            1,568\n",
      "├─BatchNorm3d: 1-8                       64\n",
      "├─MaxPool3d: 1-9                         --\n",
      "├─Conv3d: 1-10                           2,112\n",
      "├─BatchNorm3d: 1-11                      128\n",
      "├─MaxPool3d: 1-12                        --\n",
      "├─Linear: 1-13                           3,200\n",
      "├─Linear: 1-14                           33,024\n",
      "├─BatchNorm1d: 1-15                      512\n",
      "├─Dropout: 1-16                          --\n",
      "├─Dropout: 1-17                          --\n",
      "├─Dropout: 1-18                          --\n",
      "├─Linear: 1-19                           3,212,288\n",
      "├─Linear: 1-20                           524,800\n",
      "├─Linear: 1-21                           131,328\n",
      "├─Linear: 1-22                           32,896\n",
      "├─Linear: 1-23                           4,128\n",
      "├─Linear: 1-24                           33\n",
      "=================================================================\n",
      "Total params: 3,949,185\n",
      "Trainable params: 3,949,185\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            800\n",
      "├─BatchNorm2d: 1-2                       64\n",
      "├─MaxPool2d: 1-3                         --\n",
      "├─Conv2d: 1-4                            2,112\n",
      "├─BatchNorm2d: 1-5                       128\n",
      "├─MaxPool2d: 1-6                         --\n",
      "├─Conv3d: 1-7                            1,568\n",
      "├─BatchNorm3d: 1-8                       64\n",
      "├─MaxPool3d: 1-9                         --\n",
      "├─Conv3d: 1-10                           2,112\n",
      "├─BatchNorm3d: 1-11                      128\n",
      "├─MaxPool3d: 1-12                        --\n",
      "├─Linear: 1-13                           3,200\n",
      "├─Linear: 1-14                           33,024\n",
      "├─BatchNorm1d: 1-15                      512\n",
      "├─Dropout: 1-16                          --\n",
      "├─Dropout: 1-17                          --\n",
      "├─Dropout: 1-18                          --\n",
      "├─Linear: 1-19                           3,212,288\n",
      "├─Linear: 1-20                           524,800\n",
      "├─Linear: 1-21                           131,328\n",
      "├─Linear: 1-22                           32,896\n",
      "├─Linear: 1-23                           4,128\n",
      "├─Linear: 1-24                           33\n",
      "=================================================================\n",
      "Total params: 3,949,185\n",
      "Trainable params: 3,949,185\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "print(summary(model, input_shape=[(6, 10, 20), (6, 7, 7, 7), (6, 4)]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T11:58:50.987190700Z",
     "start_time": "2023-12-28T11:58:50.961193800Z"
    }
   },
   "id": "f4b1f1793e553a39"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:63\u001B[0m\n",
      "File \u001B[1;32m<timed exec>:31\u001B[0m, in \u001B[0;36mtrain_and_evaluate_model\u001B[1;34m(model, train_dataloader, test_dataloader, epochs, learning_rate)\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\GNN\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\GNN\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_and_evaluate_model(model, train_dataloader, test_dataloader, epochs, learning_rate):\n",
    "    # 定义损失函数 - 对于11分类任务，使用交叉熵损失\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 定义优化器和学习率衰减\n",
    "    l2_lambda = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)  # 每100个epoch学习率乘以0.1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch < 500: lambda_weight=0.000001\n",
    "        else: lambda_weight=0.00001\n",
    "        \n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs_2d, inputs_3d, inputs_1d, targets in train_dataloader:\n",
    "            inputs_2d, inputs_3d, inputs_1d, targets = inputs_2d.to(device), inputs_3d.to(device), inputs_1d.to(device), targets.to(device)\n",
    "            targets = targets.long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_2d, inputs_3d, inputs_1d)\n",
    "            loss = criterion(outputs, targets)\n",
    "            weight_reg = lambda_weight * (model.weight_2d.norm(2) + model.weight_3d.norm(2) + model.weight_1d.norm(2))\n",
    "            loss += weight_reg\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "\n",
    "        # 测试阶段\n",
    "        model.eval()\n",
    "        running_test_loss = 0.0\n",
    "        total_test = 0\n",
    "        correct_test = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs_2d, inputs_3d, inputs_1d, targets in test_dataloader:\n",
    "                inputs_2d, inputs_3d, inputs_1d, targets = inputs_2d.to(device), inputs_3d.to(device), inputs_1d.to(device), targets.to(device)\n",
    "                targets = targets.long()\n",
    "                outputs = model(inputs_2d, inputs_3d, inputs_1d)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += targets.size(0)\n",
    "                correct_test += (predicted == targets).sum().item()\n",
    "        avg_test_loss = running_test_loss / len(test_dataloader)\n",
    "        test_accuracy = 100 * correct_test / total_test\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Finished Training and Evaluation')\n",
    "\n",
    "# 确保你的模型定义中的最后一层是nn.Linear(32, 11)或相应的11类输出\n",
    "model = SimplifiedConvNet().to(device)\n",
    "train_and_evaluate_model(model, train_dataloader, test_dataloader, epochs=10000, learning_rate=0.01)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T02:17:31.411704300Z",
     "start_time": "2024-01-01T02:17:30.585330100Z"
    }
   },
   "id": "1da2fe739b064624"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6f420dcc5838b514"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
